<!DOCTYPE html>
<!-- saved from url=(0125)http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html -->
<html><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script type="text/javascript" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/shares.json"></script><script type="text/javascript" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/300lo.json"></script><script type="text/javascript" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/_ate.track.config_resp"></script>
    
        <title>Off-policy Learning with Linear Action Models: An Efficient One-Collection-For-All Solution - PDF</title>
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

            <meta content="Off-policy Learning with Linear Action Models: An Efficient One-Collection-For-All Solution Hengshuai Yao Department of Computing Science University of Alberta Edmonton, AB, T6G2E8" name="description">
        <meta property="og:description" content="Off-policy Learning with Linear Action Models: An Efficient One-Collection-For-All Solution Hengshuai Yao Department of Computing Science University of Alberta Edmonton, AB, T6G2E8">
    

                <link rel="amphtml" href="http://docplayer.net/amp/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html">
    


        
    <meta property="og:title" content="Off-policy Learning with Linear Action Models: An Efficient One-Collection-For-All Solution - PDF">
    <meta property="og:type" content="website">

            <meta property="og:image" content="http://docplayer.net/thumbs/55/37218908.jpg">
    
            <link rel="canonical" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html">
        <meta property="og:url" content="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html">
    
    <link rel="shortcut icon" href="http://docplayer.net/favicon.ico">

    <!-- Fonts START -->
    <link href="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/css" rel="stylesheet" type="text/css">
    <!-- Fonts END -->


    <link rel="stylesheet" type="text/css" href="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/total.css">
        <script async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/analytics.js"></script><script src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/ca-pub-8693394182295906.js"></script><script async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/adsbygoogle.js"></script>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-8693394182295906",
            enable_page_level_ads: true
        });
    </script>
    <script type="text/javascript" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/addthis_widget.js"></script><style type="text/css">.at-icon{fill:#fff;border:0}.at-icon-wrapper{display:inline-block;overflow:hidden}a .at-icon-wrapper{cursor:pointer}.at-rounded,.at-rounded-element .at-icon-wrapper{border-radius:12%}.at-circular,.at-circular-element .at-icon-wrapper{border-radius:50%}.addthis_32x32_style .at-icon{width:2pc;height:2pc}.addthis_24x24_style .at-icon{width:24px;height:24px}.addthis_20x20_style .at-icon{width:20px;height:20px}.addthis_16x16_style .at-icon{width:1pc;height:1pc}#at16lb{display:none;position:absolute;top:0;left:0;width:100%;height:100%;z-index:1001;background-color:#000;opacity:.001}#at_complete,#at_error,#at_share,#at_success{position:static!important}.at15dn{display:none}#at15s,#at16p,#at16p form input,#at16p label,#at16p textarea,#at_share .at_item{font-family:arial,helvetica,tahoma,verdana,sans-serif!important;font-size:9pt!important;outline-style:none;outline-width:0;line-height:1em}* html #at15s.mmborder{position:absolute!important}#at15s.mmborder{position:fixed!important;width:250px!important}#at15s{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAABtJREFUeNpiZGBgaGAgAjAxEAlGFVJHIUCAAQDcngCUgqGMqwAAAABJRU5ErkJggg==);float:none;line-height:1em;margin:0;overflow:visible;padding:5px;text-align:left;position:absolute}#at15s a,#at15s span{outline:0;direction:ltr;text-transform:none}#at15s .at-label{margin-left:5px}#at15s .at-icon-wrapper{width:1pc;height:1pc;vertical-align:middle}#at15s .at-icon{width:1pc;height:1pc}.at4-icon{display:inline-block;background-repeat:no-repeat;background-position:top left;margin:0;overflow:hidden;cursor:pointer}.addthis_16x16_style .at4-icon,.addthis_default_style .at4-icon,.at4-icon,.at-16x16{width:1pc;height:1pc;line-height:1pc;background-size:1pc!important}.addthis_32x32_style .at4-icon,.at-32x32{width:2pc;height:2pc;line-height:2pc;background-size:2pc!important}.addthis_24x24_style .at4-icon,.at-24x24{width:24px;height:24px;line-height:24px;background-size:24px!important}.addthis_20x20_style .at4-icon,.at-20x20{width:20px;height:20px;line-height:20px;background-size:20px!important}.at4-icon.circular,.circular .at4-icon,.circular.aticon{border-radius:50%}.at4-icon.rounded,.rounded .at4-icon{border-radius:4px}.at4-icon-left{float:left}#at15s .at4-icon{text-indent:20px;padding:0;overflow:visible;white-space:nowrap;background-size:1pc;width:1pc;height:1pc;background-position:top left;display:inline-block;line-height:1pc}.addthis_vertical_style .at4-icon,.at4-follow-container .at4-icon{margin-right:5px}html>body #at15s{width:250px!important}#at15s.atm{background:none!important;padding:0!important;width:10pc!important}#at15s_inner{background:#fff;border:1px solid #fff;margin:0}#at15s_head{position:relative;background:#f2f2f2;padding:4px;cursor:default;border-bottom:1px solid #e5e5e5}.at15s_head_success{background:#cafd99!important;border-bottom:1px solid #a9d582!important}.at15s_head_success a,.at15s_head_success span{color:#000!important;text-decoration:none}#at15s_brand,#at15sptx,#at16_brand{position:absolute}#at15s_brand{top:4px;right:4px}.at15s_brandx{right:20px!important}a#at15sptx{top:4px;right:4px;text-decoration:none;color:#4c4c4c;font-weight:700}#at15sptx:hover{text-decoration:underline}#at16_brand{top:5px;right:30px;cursor:default}#at_hover{padding:4px}#at_hover .at_item,#at_share .at_item{background:#fff!important;float:left!important;color:#4c4c4c!important}#at_share .at_item .at-icon-wrapper{margin-right:5px}#at_hover .at_bold{font-weight:700;color:#000!important}#at_hover .at_item{width:7pc!important;padding:2px 3px!important;margin:1px;text-decoration:none!important}#at_hover .at_item.athov,#at_hover .at_item:focus,#at_hover .at_item:hover{margin:0!important}#at_hover .at_item.athov,#at_hover .at_item:focus,#at_hover .at_item:hover,#at_share .at_item.athov,#at_share .at_item:hover{background:#f2f2f2!important;border:1px solid #e5e5e5;color:#000!important;text-decoration:none}.ipad #at_hover .at_item:focus{background:#fff!important;border:1px solid #fff}.at15t{display:block!important;height:1pc!important;line-height:1pc!important;padding-left:20px!important;background-position:0 0;text-align:left}.addthis_button,.at15t{cursor:pointer}.addthis_toolbox a.at300b,.addthis_toolbox a.at300m{width:auto}.addthis_toolbox a{margin-bottom:5px;line-height:initial}.addthis_toolbox.addthis_vertical_style{width:200px}.addthis_button_facebook_like .fb_iframe_widget{line-height:100%}.addthis_button_facebook_like iframe.fb_iframe_widget_lift{max-width:none}.addthis_toolbox a.addthis_button_counter,.addthis_toolbox a.addthis_button_facebook_like,.addthis_toolbox a.addthis_button_facebook_send,.addthis_toolbox a.addthis_button_facebook_share,.addthis_toolbox a.addthis_button_foursquare,.addthis_toolbox a.addthis_button_google_plusone,.addthis_toolbox a.addthis_button_linkedin_counter,.addthis_toolbox a.addthis_button_pinterest_pinit,.addthis_toolbox a.addthis_button_stumbleupon_badge,.addthis_toolbox a.addthis_button_tweet{display:inline-block}.at-share-tbx-element .google_plusone_iframe_widget>span>div{vertical-align:top!important}.addthis_toolbox span.addthis_follow_label{display:none}.addthis_toolbox.addthis_vertical_style span.addthis_follow_label{display:block;white-space:nowrap}.addthis_toolbox.addthis_vertical_style a{display:block}.addthis_toolbox.addthis_vertical_style.addthis_32x32_style a{line-height:2pc;height:2pc}.addthis_toolbox.addthis_vertical_style .at300bs{margin-right:4px;float:left}.addthis_toolbox.addthis_20x20_style span{line-height:20px}.addthis_toolbox.addthis_32x32_style span{line-height:2pc}.addthis_toolbox.addthis_pill_combo_style .addthis_button_compact .at15t_compact,.addthis_toolbox.addthis_pill_combo_style a{float:left}.addthis_toolbox.addthis_pill_combo_style a.addthis_button_tweet{margin-top:-2px}.addthis_toolbox.addthis_pill_combo_style .addthis_button_compact .at15t_compact{margin-right:4px}.addthis_default_style .addthis_separator{margin:0 5px;display:inline}div.atclear{clear:both}.addthis_default_style .addthis_separator,.addthis_default_style .at4-icon,.addthis_default_style .at300b,.addthis_default_style .at300bo,.addthis_default_style .at300bs,.addthis_default_style .at300m{float:left}.at300b img,.at300bo img{border:0}a.at300b .at4-icon,a.at300m .at4-icon{display:block}.addthis_default_style .at300b,.addthis_default_style .at300bo,.addthis_default_style .at300m{padding:0 2px}.at300b,.at300bo,.at300bs,.at300m{cursor:pointer}.addthis_button_facebook_like.at300b:hover,.addthis_button_facebook_like.at300bs:hover,.addthis_button_facebook_send.at300b:hover,.addthis_button_facebook_send.at300bs:hover{opacity:1}.addthis_20x20_style .at15t,.addthis_20x20_style .at300bs{overflow:hidden;display:block;height:20px!important;width:20px!important;line-height:20px!important}.addthis_32x32_style .at15t,.addthis_32x32_style .at300bs{overflow:hidden;display:block;height:2pc!important;width:2pc!important;line-height:2pc!important}.at300bs{overflow:hidden;display:block;background-position:0 0;height:1pc;width:1pc;line-height:1pc!important}.addthis_default_style .at15t_compact,.addthis_default_style .at15t_expanded{margin-right:4px}#at_share .at_item{width:123px!important;padding:4px;margin-right:2px;border:1px solid #fff}#at16p{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAABtJREFUeNpiZGBgaGAgAjAxEAlGFVJHIUCAAQDcngCUgqGMqwAAAABJRU5ErkJggg==);z-index:10000001;position:absolute;top:50%;left:50%;width:300px;padding:10px;margin:0 auto;margin-top:-185px;margin-left:-155px;font-family:arial,helvetica,tahoma,verdana,sans-serif;font-size:9pt;color:#5e5e5e}#at_share{margin:0;padding:0}#at16pt{position:relative;background:#f2f2f2;height:13px;padding:5px 10px}#at16pt a,#at16pt h4{font-weight:700}#at16pt h4{display:inline;margin:0;padding:0;font-size:9pt;color:#4c4c4c;cursor:default}#at16pt a{position:absolute;top:5px;right:10px;color:#4c4c4c;text-decoration:none;padding:2px}#at15sptx:focus,#at16pt a:focus{outline:thin dotted}#at15s #at16pf a{top:1px}#_atssh{width:1px!important;height:1px!important;border:0!important}.atm{width:10pc!important;padding:0;margin:0;line-height:9pt;letter-spacing:normal;font-family:arial,helvetica,tahoma,verdana,sans-serif;font-size:9pt;color:#444;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAABtJREFUeNpiZGBgaGAgAjAxEAlGFVJHIUCAAQDcngCUgqGMqwAAAABJRU5ErkJggg==);padding:4px}.atm-f{text-align:right;border-top:1px solid #ddd;padding:5px 8px}.atm-i{background:#fff;border:1px solid #d5d6d6;padding:0;margin:0;box-shadow:1px 1px 5px rgba(0,0,0,.15)}.atm-s{margin:0!important;padding:0!important}.atm-s a:focus{border:transparent;outline:0;-webkit-transition:none;transition:none}#at_hover.atm-s a,.atm-s a{display:block;text-decoration:none;padding:4px 10px;color:#235dab!important;font-weight:400;font-style:normal;-webkit-transition:none;transition:none}#at_hover.atm-s .at_bold{color:#235dab!important}#at_hover.atm-s a:hover,.atm-s a:hover{background:#2095f0;text-decoration:none;color:#fff!important}#at_hover.atm-s .at_bold{font-weight:700}#at_hover.atm-s a:hover .at_bold{color:#fff!important}.atm-s a .at-label{vertical-align:middle;margin-left:5px;direction:ltr}.at_PinItButton{display:block;width:40px;height:20px;padding:0;margin:0;background-image:url(//s7.addthis.com/static/t00/pinit00.png);background-repeat:no-repeat}.at_PinItButton:hover{background-position:0 -20px}.addthis_toolbox .addthis_button_pinterest_pinit{position:relative}.at-share-tbx-element .fb_iframe_widget span{vertical-align:baseline!important}#at16pf{height:auto;text-align:right;padding:4px 8px}.at-privacy-info{position:absolute;left:7px;bottom:7px;cursor:pointer;text-decoration:none;font-family:helvetica,arial,sans-serif;font-size:10px;line-height:9pt;letter-spacing:.2px;color:#666}.at-privacy-info:hover{color:#000}.body .wsb-social-share .wsb-social-share-button-vert{padding-top:0;padding-bottom:0}.body .wsb-social-share.addthis_counter_style .addthis_button_tweet.wsb-social-share-button{padding-top:40px}.body .wsb-social-share.addthis_counter_style .addthis_button_google_plusone.wsb-social-share-button{padding-top:0}.body .wsb-social-share.addthis_counter_style .addthis_button_facebook_like.wsb-social-share-button{padding-top:21px}@media print{#at4-follow,#at4-share,#at4-thankyou,#at4-whatsnext,#at4m-mobile,#at15s,.at4,.at4-recommended{display:none!important}}@media screen and (max-width:400px){.at4win{width:100%}}@media screen and (max-height:700px) and (max-width:400px){.at4-thankyou-inner .at4-recommended-container{height:122px;overflow:hidden}.at4-thankyou-inner .at4-recommended .at4-recommended-item:first-child{border-bottom:1px solid #c5c5c5}}</style><style type="text/css">.at-branding-logo{font-family:helvetica,arial,sans-serif;text-decoration:none;font-size:10px;display:inline-block;margin:2px 0;letter-spacing:.2px}.at-branding-logo .at-branding-icon{background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAMAAAC67D+PAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRF////+GlNUkcc1QAAAB1JREFUeNpiYIQDBjQmAwMmkwEM0JnY1WIxFyDAABGeAFEudiZsAAAAAElFTkSuQmCC")}.at-branding-logo .at-branding-icon,.at-branding-logo .at-privacy-icon{display:inline-block;height:10px;width:10px;margin-left:4px;margin-right:3px;margin-bottom:-1px;background-repeat:no-repeat}.at-branding-logo .at-privacy-icon{background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAkAAAAKCAMAAABR24SMAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAABhQTFRF8fr9ot/xXcfn2/P5AKva////////AKTWodjhjAAAAAd0Uk5T////////ABpLA0YAAAA6SURBVHjaJMzBDQAwCAJAQaj7b9xifV0kUKJ9ciWxlzWEWI5gMF65KUTv0VKkjVeTerqE/x7+9BVgAEXbAWI8QDcfAAAAAElFTkSuQmCC")}.at-branding-logo span{text-decoration:none}.at-branding-logo .at-branding-addthis,.at-branding-logo .at-branding-powered-by{color:#666}.at-branding-logo .at-branding-addthis:hover{color:#333}.at-cv-with-image .at-branding-addthis,.at-cv-with-image .at-branding-addthis:hover{color:#fff}a.at-branding-logo:visited{color:initial}.at-branding-info{display:inline-block;padding:0 5px;color:#666;border:1px solid #666;border-radius:50%;font-size:10px;line-height:9pt;opacity:.7;transition:all .3s ease;text-decoration:none}.at-branding-info span{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.at-branding-info:before{content:'i';font-family:Times New Roman}.at-branding-info:hover{color:#0780df;border-color:#0780df}</style><script type="text/javascript" charset="utf-8" async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/136.ecccad5195ab7e3df5b7.js"></script><script type="text/javascript" charset="utf-8" async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/21.740c3b67e2700152fb1e.js"></script><script type="text/javascript" charset="utf-8" async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/72.f6f5abb43e1e92724467.js"></script><script type="text/javascript" charset="utf-8" async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/16.be1983b2fafd7df82a56.js"></script><script type="text/javascript" charset="utf-8" async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/98.e378ef915c3e220d3f30.js"></script><script type="text/javascript" charset="utf-8" async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/counter.f30e91c2af45aff9c486.js"></script><style type="text/css">.addthis_counter{font-weight:700;display:inline-block;border:0;outline:0;cursor:pointer;color:#fff}.addthis_counter a{display:block;font-family:arial,helvetica,sans-serif!important;text-decoration:none!important;border:0}.addthis_counter{text-decoration:none!important;text-align:left}.addthis_counter .addthis_button_expanded{background:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIAAACaCAMAAADcrusAAAAA21BMVEX////+bUznWjrnWjrBwcGJiYnm5ubFxcWMjIyRkZH39/f19fX////nWjr+bUzys6n+uq7/7+z87evqd2H+hm3/5+P+sKPxqJz65OH/9/b+el3paU//zcT2yMDshXH99vX+kXz+nIn/w7n0vrX30cz9bEv/3tn529f/1s/ukYH3aUn+ppbvnY/bUjTmWTn/3dX+9fP3aEj/9vT52NH/5d/aUTP9a0r74tzlWDjfWz386+f/7en4b1H/7uv4b1D/3db/9/X/7uraUjT+9vT87Oj87OnpZEb52dH+dlezszuQAAAABHRSTlMAExMAzBw6IQAAAnlJREFUeF6009eKKzEMBuBJriW5T2/pPdvbqb28/xMdGbOQiyxZZ9n/QnjAH8KWJxl8uqeI3H8ZJFcUmatkFktmCUXnXHIBIRevJ/AcBuUSQI0onZ8k1/QxkNqVJIvyNPl8S1/ZMFkuuYxtqhQoS2MA1RNIGKUArjwk/4Lk0jtXp0QpjEjJHiw5SbzZQlo6eUi+fafba0985LKwqSOqJdNaMdmQ5O+NOn6WYsNlPk7nnvRF6iF3lOBz/MZqZ6kvZCBS+bUn1h/MvTCXmumYArEKijlYJr6N6t8y/fcn5zz+dfwvNlhH9ZmtB8nw6REj8vg0TO4wMnfJPpbsE4zOueQGQm5eT+A5DKopgF5htjhJLvFDIE1eoeiq0+T3A/5lw2Q65TIxmdagDU4AdIsgYJUB5NUh+RMklzbPmwwxgxVq0YLBXCBvNpBVuTgkP3/hw6UnPmLamSxHbATTRjPZouDvrT5+lm7LZTHJFp60XeYhdxTgc/zGmtxg24lAhPZrT4w/WP7CXBqmEwzEaOgWYJj4Nrp9y/Tfn5zz+Hex5Ecy3EX12e+G/9mrQxUAYBgGooEFAm3//3snZmviJnL+6cMRrXSgHqsWOGb8iISEhISEqDxRAsSld5De/nLbr4MViEEYDMI5mFQ3ff/nXejx33YHcyqtc/8QQSXaRYfJZpAaFWx2FdwfMXyqYdYjp4punpP5fcgiiyyyyCKVl9+6nxQHCP+t20WHCTdIjQo2KjgDAT/Y7UQk/li1tkPtyaQlEhBARABBUV+lvhc1rz4w+FX4bKWLrA0cp7Ue+KhqnpA/mXggAQFEBBAU9VXqe1HzzgNTv2Iyx9Dc8gWOjkKMG1wfQQAAAABJRU5ErkJggg==");background-repeat:no-repeat;display:inline-block}.addthis_counter .atc_s{background:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUBAMAAAB/pwA+AAAAFVBMVEUAAAD///////////////////////9Iz20EAAAABnRSTlMAwPm7kB4+mBDvAAAAP0lEQVQI12MIVksDgyRTBrM0KEhmAApChRnS4ACdKcCIl5ko6MAiKAZmJjAAARsKE6GAsGE43IBwJLLTER4CAHvvQkc3Hji8AAAAAElFTkSuQmCC");background-size:10px;background-repeat:no-repeat;background-color:#fe6d4c;background-position:5px 5px;display:inline-block;border-radius:2px;min-width:25px;text-decoration:none}.addthis_counter .atc_s:hover{background-color:#e75a3a}.addthis_counter.addthis_bubble_style{background-image:url("data:image/gif;base64,R0lGODlhUgBkAKIEAOrq6sLCwoWFhf///////wAAAAAAAAAAACH5BAEAAAQALAAAAABSAGQAAAP/SLoa/jDKSetkOGsdwPhgKI5kaYpAsK1sd75wPKZsjQVyrpuq7eO7YLDnqwGFyBixuDomn6UlkwOtkqTTm3ULwmYXTi7U+3WJreSp+VxNF9fs8ZcBjz/dRo8dPSfU90l4TXqAcn1/hTuCLYSJQ31gjY45i3mTipB0kpcnlTZhnDyZdKFKo2ClMJ5GqS+rLa2dpw2xorOgtR+vg7kou7y9AzSzYBbGx8gRxAwCzc7P0NHS09TSywoCm60AAsvZwSDcxALgId2z5OUf56fp6uyj7uXwmfLg9JD2wfh9+r38X76pGwBwisCBBYscRFhP2718Dh/OWThwXb+IEidi/New+SKIhD4ozmu3sRVIhSVLnVRYcWXIluNgopPZjmY8m/VwQlQnblxKRz29VRtKtCg0YsmSOsBgtGmzNz9zDCMgEkpQWGyI+KuyElebBVuhdI2jNc5JRF+pRtUBEu3XqlYSum0DNy6jPQHqcrVEVq2ds2spYQssY2xWsGZ/9MWW+NNiqo35cpmqV8jVFkqTMXVqlEnmz1I4i4Y3twplwjqCehVTthC71VxaA3qdSPYe2oVs/0WVG7HrBqgx+f1deouKylsEFDeOPPlyNM3tPpcTXSzw2oMT4QakOzJs475n8+Ye/vZ4vOV30+rN+Pf1PacBXQWdefPopgQSAAA7");background-repeat:no-repeat}.addthis_counter a.atc_s{font-size:11px;font-weight:100;color:#fff;padding:0 5px 0 20px;line-height:20px;overflow:hidden;cursor:pointer;-webkit-transition:none;transition:none}.addthis_counter .atc_s-span,.addthis_counter a.atc_s{display:block;height:20px}.addthis_counter .addthis_button_expanded.at300m .at4-icon{display:none!important}.addthis_counter a.addthis_button_expanded:hover,.addthis_counter.addthis_pill_style a.addthis_button_expanded:hover{text-decoration:none;color:#000}.addthis_counter .addthis_button_expanded{display:block;background-repeat:no-repeat;background-position:0 -40px;width:50px;height:33px;line-height:33px;padding-bottom:4px;margin-bottom:3px;text-align:center;text-decoration:none;font-size:1pc;font-weight:700;color:#333}.addthis_counter{vertical-align:top}.addthis_counter.addthis_native_counter .addthis_button_expanded{font-weight:400}* html .addthis_counter.compatmode0 .addthis_button_expanded{padding-bottom:0!important}* html .addthis_counter .addthis_button_expanded{height:37px}.addthis_counter .addthis_button_expanded:hover{background-position:0 -77px;cursor:pointer;color:#000}.addthis_counter .addthis_button_expanded .at300bs{display:none!important}.addthis_counter.addthis_pill_style{display:block;height:25px;overflow:hidden}.addthis_counter.addthis_pill_style a.atc_s{float:left}.addthis_counter.addthis_pill_style a.addthis_button_expanded{display:none;background-repeat:no-repeat;background-position:0 -114px;width:34px!important;height:20px;line-height:20px;margin:0 0 0 3px;padding:0 0 0 4px;float:left;text-align:center;text-decoration:none;font-family:arial,helvetica,sans-serif;font-weight:700;font-size:11px;color:#333;-ms-box-sizing:content-box;-o-box-sizing:content-box;box-sizing:content-box}.addthis_counter.addthis_pill_style.addthis_nonzero a.addthis_button_expanded{display:block!important;-webkit-transition:none;transition:none}.addthis_counter.addthis_pill_style a.addthis_button_expanded:hover{background-position:0 -134px!important}.addthis_counter.addthis_bubble_style{margin:0 0 0 -2px;text-align:center;font-weight:700;font-family:arial,helvetica,sans-serif;color:#000;background-repeat:no-repeat;background-position:0 -4pc;padding:0 0 0 4px;height:1pc;width:2pc!important;-ms-box-sizing:content-box;-o-box-sizing:content-box;box-sizing:content-box}.addthis_native_counter_parent .addthis_counter.addthis_bubble_style{background-position:0 -4pc!important}.addthis_counter.addthis_bubble_style.addthis_native_counter{margin:0 2px}.addthis_counter.addthis_bubble_style a.addthis_button_expanded{font-size:11px;height:1pc;line-height:1pc;width:34px;background:none}.addthis_counter.addthis_bubble_style:hover{background-position:-36px -4pc!important}.addthis_20x20_style .addthis_counter.addthis_bubble_style{background-repeat:no-repeat;background-position:0 -5pc!important;height:20px;width:35px!important;line-height:20px;padding:0 0 0 6px}.addthis_20x20_style .addthis_counter.addthis_bubble_style:hover{background-position:-41px -5pc!important}.addthis_20x20_style .addthis_counter.addthis_bubble_style a.addthis_button_expanded{background:none;font-size:9pt;line-height:20px;height:20px;margin:0;width:35px!important;padding:0!important}.addthis_20x20_style .addthis_counter.addthis_bubble_style.addthis_native_counter a.addthis_button_expanded{font-size:11px}.addthis_32x32_style .addthis_counter.addthis_bubble_style,.addthis_32x32_white_style .addthis_counter.addthis_bubble_style{background-repeat:no-repeat;background-position:0 0!important;height:2pc;width:56px!important;line-height:2pc;padding:0 0 0 6px}.addthis_32x32_style .addthis_counter.addthis_bubble_style a.addthis_button_expanded,.addthis_32x32_white_style .addthis_counter.addthis_bubble_style a.addthis_button_expanded{background:none;font-size:1pc;line-height:2pc;height:2pc;margin:0;width:56px!important;padding:0!important}.addthis_32x32_style .addthis_counter.addthis_bubble_style:hover,.addthis_32x32_white_style .addthis_counter.addthis_bubble_style:hover{background-position:0 -2pc!important}.addthis_counter.addthis_bubble_style .atc_s{display:none!important}* html .addthis_counter.addthis_bubble_style{width:36px!important;display:inline}* html .addthis_counter.bubblecompatmode0{width:2pc!important;display:block}* html .addthis_counter.addthis_bubble_style a.addthis_button_expanded{width:24px!important;height:14px!important;line-height:14px!important;padding:0;margin-top:1px!important;display:inline}* html .addthis_counter.bubblecompatmode0 a.addthis_button_expanded{width:36px}* html .addthis_32x32_style .addthis_counter.addthis_bubble_style{width:60px!important}* html .addthis_32x32_style .addthis_counter.addthis_bubble_style a.addthis_button_expanded{width:46px;height:26px!important;line-height:26px!important;margin-top:2px!important}* html .addthis_32x32_style .addthis_counter.bubblecompatmode0 a.addthis_button_expanded{height:2pc!important;line-height:2pc!important}</style></head>

<!-- Body BEGIN -->
<body class="corporate" style="">
<!-- BEGIN HEADER -->
<div class="header">
    <div class="container-fluid pre-header">
        <div class="row">
            <div class="col-md-2 hidden-sm hidden-xs additional-shop-info">
                    <a class="site-logo" href="http://docplayer.net/"><img src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/docplayer_logo.png"></a>
            </div>
            <div class="col-md-2 col-sm-3 col-xs-12 top_menu_user pull-right">
                <ul class="list-unstyled list-inline pull-right" style="float: none!important;">
                    <li class="nologin"><i class="fa fa-sign-in"></i> <a href="http://docplayer.net/login/">Log in</a></li>
                    <li class="nologin"><a href="http://docplayer.net/register/">Registration</a></li>
                </ul>
            </div>
            <div class="col-md-8 col-sm-9 col-xs-12 pull-left">
                <form action="http://docplayer.net/search/" class="content-search-view2">
                    <div class="input-group">
                        <input type="text" class="form-control" placeholder="Search..." name="q" value="">
                        <span class="input-group-btn">
                            <button type="submit" class="btn btn-primary">Search for</button>
                        </span>
                    </div>
                </form>
            </div>
        </div>
    </div>
</div>
<!-- Header END -->



    <div class="main">
        <div class="container-fluid">
            <!-- BEGIN SIDEBAR & CONTENT -->
            <div class="row margin-bottom-40">
                <!-- BEGIN CONTENT -->
                <div class="col-md-12 col-sm-12">
                    <div class="content-page">
                        <div class="row">
                            <div class="col-md-12 col-sm-12">
                                <h1 id="name_header">Off-policy Learning with Linear Action Models: An Efficient One-Collection-For-All Solution</h1>
                                                                    <div style="margin-bottom: 10px; text-align: center;">
                                        <script>
                                            var width = window.innerWidth
                                                || document.documentElement.clientWidth
                                                || document.body.clientWidth;
                                            document.write('<sc'+'ript async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></sc'+'ript> \
                       <ins class="adsbygoogle" \
                             style="display:block" \
                             data-ad-client="ca-pub-8693394182295906" \
                             data-ad-slot="4242279075" \
                             data-ad-format="auto"></ins> \
                        <sc'+'ript> \
                            (adsbygoogle = window.adsbygoogle || []).push({}); \
                        </'+'scr'+'ipt>');
                                        </script><script async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/adsbygoogle.js"></script>                        <ins class="adsbygoogle" style="display: block; height: 90px;" data-ad-client="ca-pub-8693394182295906" data-ad-slot="4242279075" data-ad-format="auto" data-adsbygoogle-status="done"><ins id="aswift_0_expand" style="display:inline-table;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:1200px;background-color:transparent"><ins id="aswift_0_anchor" style="display:block;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:1200px;background-color:transparent"><iframe width="1200" height="90" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_0" name="aswift_0" style="left:0;position:absolute;top:0;" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/saved_resource.html"></iframe></ins></ins></ins>                         <script>                             (adsbygoogle = window.adsbygoogle || []).push({});                         </script>
                                    </div>
                                                            </div>
                            <div class="col-md-9 col-sm-12">
                                <div class="row">
                                    <!-- BEGIN LEFT SIDEBAR -->
                                    <div class="col-md-12 col-sm-12 blog-item">
                                        <table style="table-layout: fixed;" width="100%">
                                            <tbody><tr>
                                                <td style="width: 160px; vertical-align: top; padding-right: 20px; padding-top: 6px;" id="left_banner_td">
                                                                                                        <div style="width: 160px; height: 600px;">
                                                        <script>
                                                            if (width < 612) {
                                                                document.getElementById('left_banner_td').style.display= "none";
                                                            } else {
                                                                document.write('<scr'+'ipt async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></scri'+'pt>\
                            <ins class="adsbygoogle"\
                                 style="display:inline-block;width:160px;height:600px"\
                                 data-ad-client="ca-pub-8693394182295906"\
                                 data-ad-slot="5719012274"></ins>\
                            <sc'+'ript>\
                                (adsbygoogle = window.adsbygoogle || []).push({});\
                            </'+'scr'+'ipt>');
                                                            }
                                                        </script><script async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/adsbygoogle.js"></script>                            <ins class="adsbygoogle" style="display:inline-block;width:160px;height:600px" data-ad-client="ca-pub-8693394182295906" data-ad-slot="5719012274" data-adsbygoogle-status="done"><ins id="aswift_1_expand" style="display:inline-table;border:none;height:600px;margin:0;padding:0;position:relative;visibility:visible;width:160px;background-color:transparent"><ins id="aswift_1_anchor" style="display:block;border:none;height:600px;margin:0;padding:0;position:relative;visibility:visible;width:160px;background-color:transparent"><iframe width="160" height="600" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_1" name="aswift_1" style="left:0;position:absolute;top:0;" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/saved_resource(2).html"></iframe></ins></ins></ins>                            <script>                                (adsbygoogle = window.adsbygoogle || []).push({});                            </script>

                                                    </div>
                                                                                                    </td>
                                                <td style="padding-left: 10px;">
                                                    <div class="blog-item-img">
                                                                                                                <iframe id="player_frame" k_dim="77" pages="9" frameborder="0" style="border-bottom: 2px solid #eee; border-top: 0px;" scrolling="no" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/saved_resource(3).html" width="100%" height="700" allowfullscreen=""></iframe>
                                                    </div>

                                                    
                                                    <div style="margin-bottom: 10px;text-align: center;">
                                                        <script>
                                                            if (width >= 1580) {
                                                                document.write('<sc'+'ript async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></sc'+'ript> \
                                    <ins class="adsbygoogle" \
                                         style="display:inline-block;width:970px;height:90px" \
                                         data-ad-client="ca-pub-8693394182295906"\
                                         data-ad-slot="7195745477"></ins>\
                                    <scr'+'ipt>\
                                        (adsbygoogle = window.adsbygoogle || []).push({});\
                                    </'+'scr'+'ipt>')
                                                            }
                                                            if (width >= 1230 && width < 1579) {
                                                                document.write('<sc'+'ript async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></sc'+'ript> \
                                    <ins class="adsbygoogle" \
                                         style="display:inline-block;width:728px;height:90px" \
                                         data-ad-client="ca-pub-8693394182295906"\
                                         data-ad-slot="8672478671"></ins>\
                                    <scr'+'ipt>\
                                        (adsbygoogle = window.adsbygoogle || []).push({});\
                                    </'+'scr'+'ipt>')
                                                            }
                                                            if (width <= 1230) {
                                                                document.write('<sc'+'ript async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></sc'+'ript> \
                                    <ins class="adsbygoogle" \
                                        style="display:inline-block;width:336px;height:280px" \
                                        data-ad-client="ca-pub-8693394182295906" \
                                        data-ad-slot="1149211879"></ins> \
                                    <scr'+'ipt>\
                                (adsbygoogle = window.adsbygoogle || []).push({}); \
                                    </'+'scr'+'ipt>')
                                                            }
                                                        </script><script async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/adsbygoogle.js"></script>                                     <ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-8693394182295906" data-ad-slot="8672478671" data-adsbygoogle-status="done"><ins id="aswift_2_expand" style="display:inline-table;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><ins id="aswift_2_anchor" style="display:block;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:728px;background-color:transparent"><iframe width="728" height="90" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_2" name="aswift_2" style="left:0;position:absolute;top:0;" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/saved_resource(4).html"></iframe></ins></ins></ins>                                    <script>                                        (adsbygoogle = window.adsbygoogle || []).push({});                                    </script>
                                                    </div>
                                                                                                    </td>
                                            </tr>
                                        </tbody></table>


                                    </div>



                                    <div class="col-md-12 col-sm-12 col-xs-12 col-lg-12">
                                        <div class="tabbable-custom">
                                            <ul class="nav nav-tabs">
                                                <li class="active">
                                                    <a href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#tab_1_1_1" data-toggle="tab">
                                                        <i class="fa fa-share-alt"></i>
                                                        SHARE </a>
                                                </li>
                                                <li class="">
                                                    <a href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#tab_1_1_2" data-toggle="tab">
                                                        <i class="fa fa-code"></i>
                                                        HTML </a>
                                                </li>
                                                <li>
                                                    <a href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#download_tab_content" data-toggle="tab" id="download_tab">
                                                        <i class="fa fa-download"></i>
                                                        DOWNLOAD </a>
                                                </li>
                                            </ul>
                                            <div class="tab-content">
                                                <div class="tab-pane active" id="tab_1_1_1">
                                                    <p>
                                                    </p><div class="addthis_toolbox addthis_default_style addthis_32x32_style">
                                                        <a class="addthis_button_facebook at300b" title="Facebook" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(59, 89, 152); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Facebook" alt="Facebook" class="at-icon at-icon-facebook" style="width: 32px; height: 32px;"><g><path d="M22 5.16c-.406-.054-1.806-.16-3.43-.16-3.4 0-5.733 1.825-5.733 5.17v2.882H9v3.913h3.837V27h4.604V16.965h3.823l.587-3.913h-4.41v-2.5c0-1.123.347-1.903 2.198-1.903H22V5.16z" fill-rule="evenodd"></path></g></svg></span></a>
                                                        <a class="addthis_button_vk at300b" target="_blank" title="Vkontakte" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(99, 131, 168); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Vkontakte" alt="Vkontakte" class="at-icon at-icon-vk" style="width: 32px; height: 32px;"><g><path d="M26.712 10.96s-.167-.48-1.21-.348l-3.447.024a.785.785 0 0 0-.455.072s-.204.108-.3.37a22.1 22.1 0 0 1-1.28 2.695c-1.533 2.61-2.156 2.754-2.407 2.587-.587-.372-.43-1.51-.43-2.323 0-2.54.382-3.592-.756-3.868-.37-.084-.646-.144-1.616-.156-1.232-.012-2.274 0-2.86.287-.396.193-.695.624-.515.648.227.036.742.143 1.017.515 0 0 .3.49.347 1.568.13 2.982-.48 3.353-.48 3.353-.466.252-1.28-.167-2.478-2.634 0 0-.694-1.222-1.233-2.563-.097-.25-.288-.383-.288-.383s-.216-.168-.527-.216l-3.28.024c-.504 0-.683.228-.683.228s-.18.19-.012.587c2.562 6.022 5.483 9.04 5.483 9.04s2.67 2.79 5.7 2.597h1.376c.418-.035.634-.263.634-.263s.192-.214.18-.61c-.024-1.843.838-2.12.838-2.12.838-.262 1.915 1.785 3.065 2.575 0 0 .874.6 1.532.467l3.064-.048c1.617-.01.85-1.352.85-1.352-.06-.108-.442-.934-2.286-2.647-1.916-1.784-1.665-1.496.658-4.585 1.413-1.88 1.976-3.03 1.796-3.52z" fill-rule="evenodd"></path></g></svg></span></a>
                                                        <a class="addthis_button_twitter at300b" title="Twitter" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(29, 161, 242); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Twitter" alt="Twitter" class="at-icon at-icon-twitter" style="width: 32px; height: 32px;"><g><path d="M27.996 10.116c-.81.36-1.68.602-2.592.71a4.526 4.526 0 0 0 1.984-2.496 9.037 9.037 0 0 1-2.866 1.095 4.513 4.513 0 0 0-7.69 4.116 12.81 12.81 0 0 1-9.3-4.715 4.49 4.49 0 0 0-.612 2.27 4.51 4.51 0 0 0 2.008 3.755 4.495 4.495 0 0 1-2.044-.564v.057a4.515 4.515 0 0 0 3.62 4.425 4.52 4.52 0 0 1-2.04.077 4.517 4.517 0 0 0 4.217 3.134 9.055 9.055 0 0 1-5.604 1.93A9.18 9.18 0 0 1 6 23.85a12.773 12.773 0 0 0 6.918 2.027c8.3 0 12.84-6.876 12.84-12.84 0-.195-.005-.39-.014-.583a9.172 9.172 0 0 0 2.252-2.336" fill-rule="evenodd"></path></g></svg></span></a>
                                                        <a class="addthis_button_amazon"></a>
                                                        <a class="addthis_button_google_plusone_share at300b" target="_blank" title="Google+" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(220, 78, 65); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Google+" alt="Google+" class="at-icon at-icon-google_plusone_share" style="width: 32px; height: 32px;"><g><path d="M12 15v2.4h3.97c-.16 1.03-1.2 3.02-3.97 3.02-2.39 0-4.34-1.98-4.34-4.42s1.95-4.42 4.34-4.42c1.36 0 2.27.58 2.79 1.08l1.9-1.83C15.47 9.69 13.89 9 12 9c-3.87 0-7 3.13-7 7s3.13 7 7 7c4.04 0 6.72-2.84 6.72-6.84 0-.46-.05-.81-.11-1.16H12zm15 0h-2v-2h-2v2h-2v2h2v2h2v-2h2v-2z" fill-rule="evenodd"></path></g></svg></span></a>
                                                        <a class="addthis_button_linkedin at300b" target="_blank" title="LinkedIn" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(0, 119, 181); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="LinkedIn" alt="LinkedIn" class="at-icon at-icon-linkedin" style="width: 32px; height: 32px;"><g><path d="M26 25.963h-4.185v-6.55c0-1.56-.027-3.57-2.175-3.57-2.18 0-2.51 1.7-2.51 3.46v6.66h-4.182V12.495h4.012v1.84h.058c.558-1.058 1.924-2.174 3.96-2.174 4.24 0 5.022 2.79 5.022 6.417v7.386zM8.23 10.655a2.426 2.426 0 0 1 0-4.855 2.427 2.427 0 0 1 0 4.855zm-2.098 1.84h4.19v13.468h-4.19V12.495z" fill-rule="evenodd"></path></g></svg></span></a>
                                                        <a class="addthis_button_pinterest_share at300b" target="_blank" title="Pinterest" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(203, 32, 39); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Pinterest" alt="Pinterest" class="at-icon at-icon-pinterest_share" style="width: 32px; height: 32px;"><g><path d="M7 13.252c0 1.81.772 4.45 2.895 5.045.074.014.178.04.252.04.49 0 .772-1.27.772-1.63 0-.428-1.174-1.34-1.174-3.123 0-3.705 3.028-6.33 6.947-6.33 3.37 0 5.863 1.782 5.863 5.058 0 2.446-1.054 7.035-4.468 7.035-1.232 0-2.286-.83-2.286-2.018 0-1.742 1.307-3.43 1.307-5.225 0-1.092-.67-1.977-1.916-1.977-1.692 0-2.732 1.77-2.732 3.165 0 .774.104 1.63.476 2.336-.683 2.736-2.08 6.814-2.08 9.633 0 .87.135 1.728.224 2.6l.134.137.207-.07c2.494-3.178 2.405-3.8 3.533-7.96.61 1.077 2.182 1.658 3.43 1.658 5.254 0 7.614-4.77 7.614-9.067C26 7.987 21.755 5 17.094 5 12.017 5 7 8.15 7 13.252z" fill-rule="evenodd"></path></g></svg></span></a>
                                                        <a class="addthis_button_tumblr at300b" target="_blank" title="Tumblr" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(55, 69, 92); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Tumblr" alt="Tumblr" class="at-icon at-icon-tumblr" style="width: 32px; height: 32px;"><g><path d="M19.59 22.176c-.392.186-1.14.348-1.695.362-1.682.045-2.008-1.18-2.022-2.07V13.93h4.218v-3.18H15.89V5.403h-3.076c-.05 0-.138.044-.15.157-.18 1.636-.947 4.51-4.133 5.66v2.71h2.124v6.862c0 2.35 1.733 5.688 6.308 5.61 1.544-.028 3.258-.674 3.637-1.23l-1.01-2.996" fill-rule="evenodd"></path></g></svg></span></a>
                                                        <a class="addthis_button_myspace at300b" target="_blank" title="Myspace" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(40, 40, 40); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Myspace" alt="Myspace" class="at-icon at-icon-myspace" style="width: 32px; height: 32px;"><g><path d="M19.996 11.757c1.905 0 3.45-1.513 3.45-3.38C23.445 6.513 21.9 5 19.995 5c-1.903 0-3.448 1.512-3.448 3.378s1.545 3.38 3.448 3.38zm4.995 5.233c-.09-2.574-2.242-4.638-4.893-4.638a4.934 4.934 0 0 0-3.24 1.206 3.62 3.62 0 0 0-3.318-2.133c-.944 0-1.8.356-2.443.935a2.596 2.596 0 0 0-2.494-1.82c-1.407 0-2.55 1.093-2.6 2.462H6v4.783h3.92v3.712h5.276V26H25v-9.01h-.01zm-11.526-6.006c1.405 0 2.545-1.116 2.545-2.492C16.01 7.115 14.87 6 13.463 6 12.06 6 10.92 7.114 10.92 8.49c0 1.376 1.14 2.492 2.544 2.492zm-4.914-.762c1.012 0 1.83-.803 1.83-1.794 0-.992-.818-1.795-1.83-1.795-1.01 0-1.83.804-1.83 1.795 0 .99.82 1.794 1.83 1.794z" fill-rule="evenodd"></path></g></svg></span></a>
                                                        <a class="addthis_button_reddit at300b" target="_blank" title="Reddit" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(255, 87, 0); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Reddit" alt="Reddit" class="at-icon at-icon-reddit" style="width: 32px; height: 32px;"><g><path d="M27 15.5a2.452 2.452 0 0 1-1.338 2.21c.098.38.147.777.147 1.19 0 1.283-.437 2.47-1.308 3.563-.872 1.092-2.06 1.955-3.567 2.588-1.506.634-3.143.95-4.91.95-1.768 0-3.403-.316-4.905-.95-1.502-.632-2.69-1.495-3.56-2.587-.872-1.092-1.308-2.28-1.308-3.562 0-.388.045-.777.135-1.166a2.47 2.47 0 0 1-1.006-.912c-.253-.4-.38-.842-.38-1.322 0-.678.237-1.26.712-1.744a2.334 2.334 0 0 1 1.73-.726c.697 0 1.29.26 1.78.782 1.785-1.258 3.893-1.928 6.324-2.01l1.424-6.467a.42.42 0 0 1 .184-.26.4.4 0 0 1 .32-.063l4.53 1.006c.147-.306.368-.553.662-.74a1.78 1.78 0 0 1 .97-.278c.508 0 .94.18 1.302.54.36.36.54.796.54 1.31 0 .512-.18.95-.54 1.315-.36.364-.794.546-1.302.546-.507 0-.94-.18-1.295-.54a1.793 1.793 0 0 1-.533-1.308l-4.1-.92-1.277 5.86c2.455.074 4.58.736 6.37 1.985a2.315 2.315 0 0 1 1.757-.757c.68 0 1.256.242 1.73.726.476.484.713 1.066.713 1.744zm-16.868 2.47c0 .513.178.95.534 1.315.356.365.787.547 1.295.547.508 0 .942-.182 1.302-.547.36-.364.54-.802.54-1.315 0-.513-.18-.95-.54-1.31-.36-.36-.794-.54-1.3-.54-.5 0-.93.183-1.29.547a1.79 1.79 0 0 0-.54 1.303zm9.944 4.406c.09-.09.135-.2.135-.323a.444.444 0 0 0-.44-.447c-.124 0-.23.042-.32.124-.336.348-.83.605-1.486.77a7.99 7.99 0 0 1-1.964.248 7.99 7.99 0 0 1-1.964-.248c-.655-.165-1.15-.422-1.486-.77a.456.456 0 0 0-.32-.124.414.414 0 0 0-.306.124.41.41 0 0 0-.135.317.45.45 0 0 0 .134.33c.352.355.837.636 1.455.843.617.207 1.118.33 1.503.366a11.6 11.6 0 0 0 1.117.056c.36 0 .733-.02 1.117-.056.385-.037.886-.16 1.504-.366.62-.207 1.104-.488 1.456-.844zm-.037-2.544c.507 0 .938-.182 1.294-.547.356-.364.534-.802.534-1.315 0-.505-.18-.94-.54-1.303a1.75 1.75 0 0 0-1.29-.546c-.506 0-.94.18-1.3.54-.36.36-.54.797-.54 1.31s.18.95.54 1.315c.36.365.794.547 1.3.547z" fill-rule="evenodd"></path></g></svg></span></a>
                                                        <a class="addthis_button_wordpress at300b" target="_blank" title="WordPress" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(88, 88, 88); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="WordPress" alt="WordPress" class="at-icon at-icon-wordpress" style="width: 32px; height: 32px;"><g><path d="M4.12 15.99c0 4.7 2.73 8.77 6.7 10.69L5.15 11.16c-.66 1.48-1.03 3.11-1.03 4.83zm19.9-.6c0-1.47-.53-2.49-.98-3.28-.6-.98-1.17-1.81-1.17-2.79 0-1.09.83-2.11 2-2.11.05 0 .102.01.15.01A11.852 11.852 0 0 0 16 4.11c-4.15 0-7.81 2.13-9.93 5.36.28.01.54.01.76.01 1.25 0 3.17-.15 3.17-.15.64-.03.72.9.07.98 0 0-.64.07-1.36.11l4.33 12.87 2.6-7.8-1.85-5.07c-.64-.04-1.25-.11-1.25-.11-.64-.04-.56-1.02.08-.98 0 0 1.96.15 3.13.15 1.24 0 3.17-.15 3.17-.15.64-.03.72.9.07.98 0 0-.64.07-1.36.11l4.3 12.77 1.19-3.96c.6-1.54.9-2.82.9-3.84zm-7.81 1.64l-3.57 10.36a11.967 11.967 0 0 0 7.3-.19c-.03-.05-.06-.1-.08-.16l-3.65-10.01zm10.22-6.74c.05.38.08.78.08 1.22 0 1.2-.23 2.56-.9 4.26l-3.63 10.49c3.53-2.06 5.91-5.89 5.91-10.27-.01-2.06-.54-4.01-1.46-5.7z"></path><g></g><path d="M12.55 11.31s.6.08 1.25.11l1.68 4.6.17-.52-1.85-5.07c-.3-.02-.58-.04-.81-.07-.22-.02-.36-.02-.36-.02-.65-.05-.72.93-.08.97zM9.05 11.4c.57-.04 1.03-.09 1.03-.09.64-.08.56-1.02-.07-.98 0 0-.21.02-.52.04-.23.02-.49.03-.77.05l.33.98zM22.43 25.96l3.18-9.19c.68-1.69.9-3.05.9-4.25 0-.15-.02-.28-.03-.43-.06 1.06-.3 2.25-.88 3.68l-3.63 10.49c.16-.09.3-.2.46-.3zM27.85 16.48c.01-.16.03-.32.03-.48 0-2.07-.53-4.01-1.45-5.7.05.36.07.75.08 1.17.79 1.5 1.26 3.2 1.34 5.01zM16.21 17.03l-3.57 10.36c.1.03.21.05.32.08l3.25-9.44 3.39 9.27c.12-.04.24-.07.35-.11a.79.79 0 0 1-.08-.16l-3.66-10zM18.92 10.33s-.21.02-.52.04c-.22.02-.49.04-.77.06l.33.98c.568-.04 1.03-.09 1.03-.09.65-.09.57-1.03-.07-.99zM4.12 15.99c0 .2.01.4.02.6.05-1.57.4-3.07 1.01-4.43l5.22 14.29.45.24-5.67-15.53c-.66 1.48-1.03 3.11-1.03 4.83zM16 5.11c2.63 0 5.06.86 7.02 2.31.25-.12.53-.2.85-.2.05 0 .102.01.15.01A11.813 11.813 0 0 0 16 4.11c-4.15 0-7.81 2.13-9.93 5.36.27.01.52.01.74.01C8.99 6.82 12.29 5.11 16 5.11zM23.04 13.12c.4.7.85 1.61.94 2.83.02-.19.04-.39.04-.56 0-1.47-.53-2.49-.98-3.28-.5-.81-.95-1.52-1.1-2.3-.04.17-.07.34-.07.51.01.99.57 1.82 1.17 2.8z"></path></g></svg></span></a>
                                                        <a class="addthis_button_blogger at300b" target="_blank" title="Blogger" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(245, 125, 0); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Blogger" alt="Blogger" class="at-icon at-icon-blogger" style="width: 32px; height: 32px;"><g><path d="M19.864 21.38H11.84a1.712 1.712 0 0 1 0-3.425h8.024a1.712 1.712 0 0 1 0 3.425zm-7.542-11.27l4.012.063a1.712 1.712 0 0 1-.054 3.424l-4.012-.064a1.712 1.712 0 0 1 .054-3.424zm13.4 9.404c-.007-.374-.008-.71-.01-1.014-.006-1.58-.012-2.83-1.016-3.803-.716-.694-1.565-.914-2.855-.962.176-.747.226-1.575.145-2.47-.02-2.973-2.234-5.18-5.304-5.264h-.043l-4.692.072c-1.844-.007-3.3.53-4.332 1.606-.638.666-1.362 1.83-1.45 3.72H6.16v.057a8.6 8.6 0 0 0-.006.393l-.12 7.125c-.008.143-.015.288-.016.437-.12 2.088.372 3.728 1.463 4.876 1.078 1.132 2.664 1.706 4.715 1.706H19.516c1.84-.017 3.393-.624 4.494-1.757 1.1-1.132 1.692-2.743 1.713-4.66v-.06z" fill-rule="evenodd"></path></g></svg></span></a>
                                                        <a class="addthis_button_adifiny"></a>
                                                        <a class="addthis_button_vk at300b" target="_blank" title="Vkontakte" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(99, 131, 168); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Vkontakte" alt="Vkontakte" class="at-icon at-icon-vk" style="width: 32px; height: 32px;"><g><path d="M26.712 10.96s-.167-.48-1.21-.348l-3.447.024a.785.785 0 0 0-.455.072s-.204.108-.3.37a22.1 22.1 0 0 1-1.28 2.695c-1.533 2.61-2.156 2.754-2.407 2.587-.587-.372-.43-1.51-.43-2.323 0-2.54.382-3.592-.756-3.868-.37-.084-.646-.144-1.616-.156-1.232-.012-2.274 0-2.86.287-.396.193-.695.624-.515.648.227.036.742.143 1.017.515 0 0 .3.49.347 1.568.13 2.982-.48 3.353-.48 3.353-.466.252-1.28-.167-2.478-2.634 0 0-.694-1.222-1.233-2.563-.097-.25-.288-.383-.288-.383s-.216-.168-.527-.216l-3.28.024c-.504 0-.683.228-.683.228s-.18.19-.012.587c2.562 6.022 5.483 9.04 5.483 9.04s2.67 2.79 5.7 2.597h1.376c.418-.035.634-.263.634-.263s.192-.214.18-.61c-.024-1.843.838-2.12.838-2.12.838-.262 1.915 1.785 3.065 2.575 0 0 .874.6 1.532.467l3.064-.048c1.617-.01.85-1.352.85-1.352-.06-.108-.442-.934-2.286-2.647-1.916-1.784-1.665-1.496.658-4.585 1.413-1.88 1.976-3.03 1.796-3.52z" fill-rule="evenodd"></path></g></svg></span></a>
                                                        <a class="addthis_button_ziczac at300b" target="_blank" title="ZicZac" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(255, 137, 31); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="ZicZac" alt="ZicZac" class="at-icon at-icon-ziczac" style="width: 32px; height: 32px;"><g><g fill-rule="evenodd"></g><path d="M17.882 18.067c1.378-.543 8.918-3.613 8.918-6.947-.946.31-11.09 5.49-11.09 5.49-.197-.395-1.308-12.352-1.505-12.154-2.608 2.605-1.052 9.81-1.052 11.666 0 1.693-.44 2.086-1.837 2.428-.023-.027-.044-.047-.068-.07a3.187 3.187 0 0 0-1.195-1.43 2.29 2.29 0 0 0-1.245-.47c-.152-.017-.305-.036-.462-.036a3.144 3.144 0 1 0 2.726 4.71.036.036 0 0 1 .017-.017c.286-.414.544-.906.943-1.148.508-.312 1.43-.72 1.668-.513.492.432.854 1.238.886 1.902.023.464-.183.933-.382 1.405-.22.398-.352.86-.378 1.342a1.688 1.688 0 0 0-.008.174c-.007.296.033.577.117.845a3.142 3.142 0 0 0 5.91.412c.304-.61.37-1.312.17-2.123-.258-1.06-.815-1.61-1.526-1.885a3.17 3.17 0 0 0-1.526-.396c-.184 0-.36.02-.537.053-.76-2.35-.735-2.372 1.46-3.24zm-9.537 3.15a1.526 1.526 0 1 1 0-3.05 1.526 1.526 0 0 1 0 3.05zm8.616 1.655a1.528 1.528 0 0 1 0 3.056 1.528 1.528 0 0 1 0-3.056z"></path><path d="M16.804 4.9h5.75l-3.318 5.935h3.097v1.547h-5.935l3.245-5.934h-2.84V4.9zM5.605 6.583h5.75L8.037 12.52h3.096v1.545H5.2L8.444 8.13h-2.84V6.583z"></path></g></svg></span></a>
                                                        <a class="addthis_button_favorites at300b" title="Favorites" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(245, 202, 89); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Favorites" alt="Favorites" class="at-icon at-icon-favorites" style="width: 32px; height: 32px;"><g><path d="M26.56 13.56a.432.432 0 0 0-.4-.29h-7.51l-2.32-7.14c-.06-.17-.22-.28-.39-.28s-.34.11-.39.28l-2.34 7.14H5.72c-.18 0-.34.12-.39.29-.06.17.01.35.15.46l6.06 4.42-2.34 7.17c-.06.17.01.35.15.46.14.11.34.1.49 0l6.1-4.43 6.09 4.43c.07.05.16.08.24.08s.17-.03.24-.08c.15-.1.2-.29.15-.46l-2.34-7.18 6.08-4.42a.37.37 0 0 0 .16-.45z"></path></g></svg></span></a>
                                                        <a class="addthis_button_gmail at300b" target="_blank" title="Gmail" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(219, 68, 55); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Gmail" alt="Gmail" class="at-icon at-icon-gmail" style="width: 32px; height: 32px;"><g><g fill-rule="evenodd"></g><path opacity=".3" d="M7.03 8h17.94v17H7.03z"></path><path d="M7.225 8h-.41C5.815 8 5 8.84 5 9.876v13.248C5 24.16 5.812 25 6.815 25h.962V12.714L16 19.26l8.223-6.546V25h.962C26.188 25 27 24.16 27 23.124V9.876C27 8.84 26.186 8 25.185 8h-.41L16 15.506 7.225 8z"></path></g></svg></span></a>
                                                        <a class="addthis_button_buzzzy"></a>
                                                        <a class="addthis_button_technerd at300b" target="_blank" title="Communicate" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(49, 104, 150); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="Communicate" alt="Communicate" class="at-icon at-icon-technerd" style="width: 32px; height: 32px;"><g><path d="M6.13 7.02c5.07.14 8.82.2 11.26.2 1.85 0 3.88-.05 6.1-.16.79-.04 1.44-.06 1.95-.06.19 0 .52.01.98.01.07.4.16 1.23.26 2.48l.22 2.31c0 .38-.16.57-.48.57-.24 0-.48-.23-.7-.7-.37-.79-.85-1.47-1.43-2.04-.58-.57-1.26-.94-2.02-1.13s-2.03-.28-3.8-.28c-.16 1.49-.23 3.36-.23 5.61 0 2.3.04 4.51.13 6.64.09 2.14.19 3.42.32 3.86.13.44.34.76.65.97s1.02.35 2.14.42c.38.03.57.17.57.41 0 .27-.17.41-.5.41l-.41-.01-3.03-.07c-2.37 0-3.8.01-4.31.03l-1.85.07c-.15 0-.45-.01-.91-.03-.23-.05-.34-.17-.34-.37 0-.28.19-.42.56-.42.87 0 1.52-.18 1.96-.54.44-.36.71-.93.81-1.72.1-.78.15-2.49.15-5.12 0-4.15-.13-7.53-.38-10.14-1.61 0-2.78.06-3.49.2-.73.15-1.42.51-2.09 1.1-.67.59-1.21 1.35-1.63 2.29-.17.37-.36.56-.57.56-.33 0-.5-.15-.5-.44 0-.09.06-.73.18-1.93.13-1.19.27-2.19.43-2.98z"></path></g></svg></span></a>
                                                        <a class="addthis_button_compact at300m" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#"><span class="at-icon-wrapper" style="background-color: rgb(255, 101, 80); line-height: 32px; height: 32px; width: 32px;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 32 32" title="More" alt="More" class="at-icon at-icon-addthis" style="width: 32px; height: 32px;"><g><path d="M18 14V8h-4v6H8v4h6v6h4v-6h6v-4h-6z" fill-rule="evenodd"></path></g></svg></span></a><a class="addthis_counter addthis_bubble_style" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#" style="display: inline-block;"><a class="addthis_button_expanded" target="_blank" title="More" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#">0</a><a class="atc_s addthis_button_compact">Share<span></span></a></a>
                                                    <div class="atclear"></div></div>
                                                    <p></p>
                                                    <p></p>
                                                </div>
                                                <div class="tab-pane" id="tab_1_1_2" style="padding-top: 20px; padding-bottom: 20px;">
                                                    <div id="size_slider" class="slider bg-blue ui-slider ui-slider-horizontal ui-widget ui-widget-content ui-corner-all" aria-disabled="false" w="728" h="946">
                                                        <a class="ui-slider-handle ui-state-default ui-corner-all bg-red" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#" style="left: 59.116%;"></a>
                                                    <div class="ui-slider-range ui-widget-header ui-corner-all ui-slider-range-min" style="width: 59.116%;"></div></div>
                                                    <div class="row margin-top-10 margin-bottom-10" style="line-height: 27px; font-size: 20px; font-weight: 100;">
                                                        <div class="col-md-5 col-sm-12">
                                                            Size: <span class="slider-value" id="slider-vertical-amount">728x946</span>px
                                                        </div>
                                                        <div class="col-md-5 col-sm-6" style="text-align: right; line-height: 27px; font-size: 20px; font-weight: 100;">
                                                            Start display at page:
                                                        </div>
                                                        <div class="col-md-2 col-sm-6">
                                                            <select class="form-control" id="embed_page_start">
                                                            <option value="1">1</option><option value="2">2</option><option value="3">3</option><option value="4">4</option><option value="5">5</option><option value="6">6</option><option value="7">7</option><option value="8">8</option><option value="9">9</option></select>
                                                        </div>
                                                    </div>
                                                    <p>
                                                        <textarea id="embed_code" class="form-control" rows="3"></textarea>
                                                    </p>
                                                </div>
                                                <div class="tab-pane" id="download_tab_content" style="min-height: 100px;">
                                                    <strong>Download "Off-policy Learning with Linear Action Models: An Efficient One-Collection-For-All Solution"</strong>

                                                    <form action="http://docplayer.net/doc/get-download-link/" id="form_download">
                                                        <div id="download_loading" style="text-align: center; margin-bottom: 20px; margin-top: 20px;"><img src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/loading.gif" border="0" width="64" height="64"></div>
                                                        <div class="note note-danger" id="download_error_message" style="display: none;">
                                                            <p>
                                                                Error: <span></span>
                                                            </p>
                                                        </div>

                                                        <input type="hidden" name="doc_id" value="37218908">
                                                        <a id="download_link" download="Off-policy Learning with Linear Action Models: An Efficient One-Collection-For-All Solution.pdf" class="btn bg-purple-seance" href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#" target="_blank" style="margin-top: 10px; display: none;">
                                                            <i class="fa fa-download"></i> Download Document</a>
                                                    </form>
                                                </div>
                                                                                                    <div style="margin-top: 10px; text-align: center;">
                                                    <script>
                                                        if (width <= 611) {
                                                            document.write('<sc'+'ript async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></sc'+'ript> \
                                    <ins class="adsbygoogle" \
                                        style="display:inline-block;width:320px;height:100px" \
                                        data-ad-client="ca-pub-8693394182295906" \
                                        data-ad-slot="2567069477"></ins> \
                                    <scr'+'ipt>\
                                (adsbygoogle = window.adsbygoogle || []).push({}); \
                                    </'+'scr'+'ipt>')
                                                        }
                                                        if (width > 611 && width < 1000) {
                                                            document.write('<sc'+'ript async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></sc'+'ript> \
                                    <ins class="adsbygoogle" \
                                        style="display:inline-block;width:336px;height:280px" \
                                        data-ad-client="ca-pub-8693394182295906" \
                                        data-ad-slot="1090336272"></ins> \
                                    <scr'+'ipt>\
                                (adsbygoogle = window.adsbygoogle || []).push({}); \
                                    </'+'scr'+'ipt>')
                                                        }
                                                        if (width > 1001 ){
                                                            document.write('<sc'+'ript async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></sc'+'ript> \
                                                        <ins class="adsbygoogle" \
                                                             style="display:block;"\
                                                             data-ad-client="ca-pub-8693394182295906"\
                                                             data-ad-slot="4183403475"\
                                                             data-ad-format="auto"></ins>\
                                                        <scr'+'ipt>\
                                                            (adsbygoogle = window.adsbygoogle || []).push({});\
                                                        </'+'scr'+'ipt>')
                                                        }
                                                    </script><script async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/adsbygoogle.js"></script>                                                         <ins class="adsbygoogle" style="display: block; height: 90px;" data-ad-client="ca-pub-8693394182295906" data-ad-slot="4183403475" data-ad-format="auto" data-adsbygoogle-status="done"><ins id="aswift_3_expand" style="display:inline-table;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:1012px;background-color:transparent"><ins id="aswift_3_anchor" style="display:block;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:1012px;background-color:transparent"><iframe width="1012" height="90" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_3" name="aswift_3" style="left:0;position:absolute;top:0;" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/saved_resource(5).html"></iframe></ins></ins></ins>                                                        <script>                                                            (adsbygoogle = window.adsbygoogle || []).push({});                                                        </script>
                                                </div>
                                                                                            </div>
                                        </div>
                                    </div>
                                    <div class="col-md-12 col-sm-12 col-xs-12 blog-item">
                                        <ul class="blog-info">
                                            <li><i class="fa fa-user"></i> <a href="http://docplayer.net/user/37759041/"> Gertrude Chandler</a></li>
                                            <li><i class="fa fa-calendar"></i> 3 months ago                                                                                            </li>
                                            <li><i class="fa fa-eye"></i> Views: <span id="views_count" doc_id="37218908">1</span> </li>
                                        </ul>
                                    </div>

                                    <!--    <div class="col-md-12 col-sm-12 col-xs-12 blog-item">-->
                                    <!--        <h2>-->
                                    <!--            <i class="fa fa-comments-o"></i>-->
                                    <!--            --><!--</h2>-->
                                    <!--        <div class="comments" style="overflow: hidden;">-->
                                    <!--            <div class="fb-comments" data-href="--><!--" data-numposts="5" data-width="100%"></div>-->
                                    <!--        </div>-->
                                    <!--    </div>-->

                                    <div class="col-md-12 col-sm-12 col-sm-12 col-xs-12" id="transcript_block">
                                        <h2 style="margin-top: 30px;"><i class="fa fa-file-text-o"></i>Transcription</h2>
                                                                                    <div id="ban_200x200">
                                                <script>
                                                    if (width <= 611) {
                                                        document.write('<sc'+'ript async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></sc'+'ript> \
                                    <ins class="adsbygoogle" \
                                        style="display:inline-block;width:125px;height:125px" \
                                        data-ad-client="ca-pub-8693394182295906" \
                                        data-ad-slot="9388908679"></ins> \
                                    <scr'+'ipt>\
                                (adsbygoogle = window.adsbygoogle || []).push({}); \
                                    </'+'scr'+'ipt>')
                                                    }
                                                    if (width > 612){
                                                        document.write('<sc'+'ript async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></sc'+'ript> \
                                    <ins class="adsbygoogle" \
                                        style="display:inline-block;width:200px;height:200px" \
                                        data-ad-client="ca-pub-8693394182295906" \
                                        data-ad-slot="1229937072"></ins> \
                                    <scr'+'ipt>\
                                (adsbygoogle = window.adsbygoogle || []).push({}); \
                                    </'+'scr'+'ipt>')
                                                    }
                                                    </script><script async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/adsbygoogle.js"></script>                                     <ins class="adsbygoogle" style="display:inline-block;width:200px;height:200px" data-ad-client="ca-pub-8693394182295906" data-ad-slot="1229937072" data-adsbygoogle-status="done"><ins id="aswift_4_expand" style="display:inline-table;border:none;height:200px;margin:0;padding:0;position:relative;visibility:visible;width:200px;background-color:transparent"><ins id="aswift_4_anchor" style="display:block;border:none;height:200px;margin:0;padding:0;position:relative;visibility:visible;width:200px;background-color:transparent"><iframe width="200" height="200" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_4" name="aswift_4" style="left:0;position:absolute;top:0;" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/saved_resource(6).html"></iframe></ins></ins></ins>                                     <script>                                (adsbygoogle = window.adsbygoogle || []).push({});                                     </script>
                                            </div>
                                                                                <p><span class="badge bg-blue-hoki">1</span> Off-policy Learning with Linear Action Models: An Efficient One-Collection-For-All Solution Hengshuai Yao Department of Computing Science University of Alberta Edmonton, AB, T6G2E8 Abstract We propose a model-based off-policy learning method that can be used to evaluate any target policy using data collected from arbitrary sources. The key of this method is a set of linear action models (LAM) learned from data. The method is simple to use. First, a target policy tells what actions are taken at some features and LAM project what would happen for the actions. Second, a convergent off-policy learning algorithm such as LSTD and gradient TD algorithms evaluates the projected experience. We focus on two off-policy learning algorithms with LAM, i.e., the stochastic LAM-LSTD and the deterministic LAM-LSTD. Empirical results show that the two LAM-LSTD algorithms give more accurate predictions for various target policies than the on-policy LSTD learning. LAM based off-policy learning algorithms are also exclusively useful in difficult control tasks where one could not collect sufficient on-policy samples for on-policy learning. This work leads us to advocate using off-policy learning to evaluate many policies in place of on-policy learning, improving the efficiency of using data.. Introduction Off-policy learning, which aims to evaluate a policy based on the data generated/collected from another policy, is an interesting problem in reinforcement learning (RL) (Watkins, 989; Sutton &amp; Barto, 998; Lagoudakis &amp; Parr, 23). Off-policy learning Appearing in Planning and Acting with Uncertain Models Workshop at the 28 th ICML, Bellevue, WA, USA, 2. Copyright 2 by the author(s)/owner(s). is a very important way of increasing our knowledge about the world. The amazing feature of off-policy learning is that a single stream of data from arbitrary sources can provide us knowledge about many policies. Therefore off-policy learning is an important way of improving the efficiency of using sample. Though charming in definition, learning many policies from a single-stream of data has rarely been practiced in RL, mainly because off-policy learning has pitfalls and is inherently hard. The difficulty lies in that the distribution of the data is from the policy that generates/collects the data, which causes many RL algorithms to diverge (Sutton &amp; Barto, 998). Importance sampling was first proposed to correct the distribution of data for off-policy learning (Precup et al., 2). However, it has a high variance in estimation. Recently, several proposed gradient temporal difference (TD) methods with linear function approximation are proved to converge for off-policy learning (Sutton et al., 29). In this paper, we study off-policy learning in an offline setting, in which a data set of samples are collected before hand using an arbitrary policy. We propose a general approach for off-policy learning, which stands out from existing off-policy learning solutions in that it is model-based. The key component of this framework is a set of approximate action models with linear function approximation, which are called linear action models(lam) for short. LAM belong to the family of linear models. Boyan built a compressed model for the prediction problem, and gave a new interpretation for the previous LSTD (Bradtke &amp; Barto, 996), and exted it to eligibility traces (Boyan, 22). It was shown that the fixed points of model-free and model-based value function approximation are equivalent given the same set of features (Parr et al., 28). LAM differs from these models in that it models the effects of actions with linear function approximation. LAM was first explored in a linear Dyna algorithm</p><p><span class="badge bg-blue-hoki">2</span> for online planning and control (Sutton et al., 28). In their algorithm, learning, modeling and planning proceed simultaneously. Action selection in learning and planning is performed according to LAM. However, their paper focused mainly on the on-policy prediction problem, and LAM were only briefly studied. They also used a gradient descent algorithm to learn LAM, which is slow and requires tuning a step-size. The gradient descent method is problematic also in that the induced LAM can be biased because of the choice of the step-size parameter. Apparently LAM is policy-indepdent. It is this policy indepence property that makes it suitable for off-policy learning. Because of policy indepence, LAM-based off-policy learning does not have to use importance sampling to correct the behavior/collection policy for the target policy. So LAM is free of the high variance caused by importance sampling. Because of policy indepence, LAM-based off-policy learning considers the target policy not at the time of modeling, but only when learning is requested. This clear separation of modeling from learning is the key to the simplicity and effectiveness of the proposed off-policy learning solution. LAM-based off-policy learning is simple to use. First, LAM are learned from a given set of samples with some chosen features, using an efficient least-squares method that can guarantee the quality of LAM. Second, a target policy tells what actions are taken on some We then apply LAM to project what would happen from these features according to the policy. Third, we use an algorithm to evaluate the projected experience. Notice the learning is off-policy, so we need convergent off-policy learning algorithms such as gradient TD algorithms or LSTD algorithm. We focus on the use of LSTD since it is efficient in using samples, and does not require tuning a step-size. However, other learning algorithms such as gradient TD can also be used to evaluate the projected experience by LAM. The emphasis of this paper is not on the comparisons between LSTD and gradient TD since it is already well known that least-squares methods are more data efficient (Bradtke &amp; Barto, 996; Boyan, 22; Xu et al., 22). We demonstrate that off-policy learning can be much more accurate than on-policy learning. Our two offpolicy learning algorithms perform very well in evaluating various target policies. In particular, for those policies that are ill distributed, according to which some actions are rarely taken or some states are rarely visited, the advantages of our algorithms are very pronounced. Notice that the problem is inherently hard because of the rareness caused by the nature of these policies. In a related rareness problem studied by Algorithm Learning LAM from a set of samples using a least-squres method. Input: a data set, D = {&lt; φ i, a i, φ i+, r i &gt;}, or D s = {(s i, a i, s i+, r i &gt;}. Output: a set of LAM, {&lt; F a, f a &gt;}. Initialize H a, E a and e a for all a for i =, 2,...,d do Read the transition: &lt; φ i, a i, φ i+, r i &gt; if using D s Set φ i = φ(s i ), φ i+ = φ(s i+ ) for a = a i, update LAM of a by H a = H a + φ i φ T i E a = E a + φ i+ φ T i e a = e a + φ i r i for all a, solve LAM by least-squares: F a d = Ea (H a ) f a d = (Ha ) e a (Frank et al., 28), rare events occur indepently of actions. The rareness we study in this paper is caused by the policies themselves, and is much more common in RL. 2. Learning LAM Suppose the state space is denoted by S, and we have N states. First we are given a data set of samples, D = {&lt; φ i, a i, φ i+, r i &gt;}, where φ i is some feature at which action a i is taken, φ i+ is the resulting feature, and r i is the resulting reward, i =, 2,...,d, d = D. The samples can be collected from a single policy or many different policies, by a single agent or many different agents. There is no restriction on the data set. However, to guarantee the quality of LAM and offpolicy learning, the data set should contain sufficient samples. LAM are learned using linear function approximation. Given n (n N) feature functions ϕ j ( ) : S R, j =,..., n, the feature vector (feature for short) of state i is φ(i) = [ϕ (i), ϕ 2 (i),..., ϕ n (i)] T. Let Φ be the feature matrix whose entries are Φ i,j = ϕ j (i), i =,..., N; j =,...,n. We assume the columns of Φ are linearly indepent. Each LAM is composed of a matrix and a vector, &lt; F a n n, f a n &gt;, where F a approximates the transition dynamics and f a approximates the rewards of taking action a in the feature Notice that the data set can also be the experience of transitioning among states, D s = {&lt; s i, a i, s i+, r i &gt;}, where s i, s i+ S.</p><p><span class="badge bg-blue-hoki">3</span> Algorithm 2 The stochastic LAM-LSTD algorithm: a simulation-based-projection on/off-policy learning with the pre-learned LAM. No iteration is required. Input: a data set of features, D φ = {φ i }, a set of LAM, {&lt; F a, f a &gt;} learned from D; and a target policy π. Output: a parameter vector θ for policy π. for i =, 2,...,d do Read φ i Select an action a according to π at φ i /* for on-policy learning, a = a i */ φ i+ = F a φ i r i = φ T i fa A = A + φ i (γ φ i+ φ i ) T b = b + φ i r i θ = A b space. Algorithm shows an efficient least-squares method of learning LAM. 3. Off-policy Learning Algorithms with LAM 3.. A Simulation-based Method The first algorithm, a simulation-based method, is shown in Algorithm 2. The algorithm is run on a data set of features, D φ = {φ i }. Thus at this stage the transitioning experience is no longer necessary. In the experiments, we set D φ to be the set of features where the transitioning samples were collected from. That is, D φ = {φ i φ i D}. This, however, is not a constraint, as one can choose freely for D φ. Notice that F a φ is the expected next feature, and φ T f a is the expected reward of taking a at φ. To evaluate a policy with LAM, one follows the policy, generating an action a at a feature φ, and do the projection operation, which gives the imaginary transition experience, &lt; φ, φ T f a, F a φ &gt;. Notice that evaluating the imaginary experience is an off-policy learning problem. To guarantee convergence and data efficiency, we use LSTD for policy evaluation in Algorithm 2. Notice that Algorithm 2 can also be used for on-policy learning, in which the target policy π is also the policy we used to collect the samples, i.e., π(φ i ) = a i, i =, 2,...,d. On-policy learning can then be based on the projected experience under the action selected according to the policy. For off-policy learning, Algorithm 2 can be used to evaluate a target policy known beforehand, or a target Algorithm 3 The deterministic LAM-LSTD algorithm: an analytical-projection based off-policy learning with the pre-learned LAM. No iteration or simulation is required. Input, Output: the same as Algorithm 2 for i =, 2,...,d do Read φ i Set φ π i+ =, rπ i = for each action a φ π i+ = φ π i+ + π(φ i, a)f a φ i r π i = r π i + π(φ i, a)φ T i fa A = A + φ i (γ φ π i+ φ i) T b = b + φ i r π i θ = A b policy only known when processing samples, such as greedy policies. In a recent paper (Yao, 2), we proposed approximate policy iteration using LAM. In that case, learning is still off-policy, and the goal is to evaluate the greedy/optimal policy. The focus of this paper is on evaluation of various policies that are generally not greedy or optimal A Deterministic Method If the target policy is known, Algorithm 2 can be made more efficient in projection. We can generate the next feature and reward for a given feature under the target policy, by taking advantage of the target policy and LAM. Given a feature, we can project the experience under the target policy at once, without simulating step by step. In particular, for a feature φ i, the expected next feature according to policy π is and the reward is φ π i+ = a r π i = a π(φ i, a)f a φ i, π(φ i, a)φ T i f a. Algorithm 3 shows this more efficient method. The algorithm is deterministic and does not require any simulation. The algorithm can also be used for on-policy learning if the collection policy (which is π) is known. However, in practice, the samples may be from various sources (e.g., collected by many agents following different policies), and hence the collection policy can be unknown. In this case, Algorithm 3 is not applicable for on-policy learning, and one has to use Algorithm 2.</p><p><span class="badge bg-blue-hoki">4</span> 4. Empirical Results 4.. Boyan MDP The problem is slightly modified from Boyan chain (Boyan, 22). We interpret it as a MDP problem. At each state, there are two actions available. Action a walks the state i to state i. Action a 2 jumps state i to state i 2 except that at state it takes the agent to state. Both actions are deterministic: taking an action leads to the inted state without any problem. We consider evaluating the following three target policies: policy : walking with probability 5%, and jumping with 5% at each state (this is the original policy of Boyan chain); and policy 2: walking with probability 9%, and jumping with % at each state; policy 3: walking with probability.%, and jumping with % at each state. We compared on/off-policy learning of the three policies. For on-policy learning of a policy, samples were collected from a number of episodes following the policy. For off-policy learning, samples were collected in a number of episodes following a purely random policy (taking uniformly random actions at each state). For both on-policy and off-policy learning, episodes of samples were collected. All episodes start from state 2 and terminate in state. In both on-policy and off-policy evaluation, LSTD was used. For off-policy learning, two LAM were first learned from the samples using Algorithm. Then we projected features φ i in the samples and applied LSTD to evaluate the projected experience. We also compared the two ways of projecting experience: the stochastic way (Algorithm 2) and the deterministic way (Algorithm3). Notice that the original features by Boyan can only represent the value function of policy. In order to represent all the policies exactly, we also used the tabular features in addition to the original linear interpolation Figure shows the results of evaluating policy using the original linear For this policy, onpolicy learning and the stochastic LAM-LSTD have a similar convergence rate, for two reasons. First, the state/action distribution under policy is very smooth, and the learned LAM do not provide a significant advantage in the coverage of state space over on-policy learning. Second, they both dep on the sampling of the policy in policy evaluation. However, the deterministic LAM-LSTD wins on the second aspect. The deterministic LAM-LSTD does not dep 2 stochastic LAM LSTD Number of Episodes (Log) Figure. The Boyan MDP: learning policy with the linear on the sampling of the policy in projecting experience for policy evaluation, and the convergence is very fast seen from the figure. The case of tabular features is similar and thus the figure is not included here. Figure 2 shows the results of evaluating policy 2 using the linear interpolation For this policy, the state/action distribution under the policy is not very smooth. Typically, the jumping action is more frequently taken, and half of the states are more frequently visited. Hence the accuracy of on-policy learning is bottlenecked by those infrequently visited states. The LAM have a much finer accuracy because it is learned from the data collected from the random policy which is almost uniformly distributed in both states and actions. Thus both the stochastic and deterministic LAM-LSTD converge faster than on-policy learning. The two have a similar convergence rate because the RMSE quickly achieves the bound that is enforced by the Figure 3 shows the results of using the tabular This time the deterministic LAM-LSTD is much faster than the stochastic LAM- LSTD, since the tabular features can represent value functions exactly. We continue to learning policy 3. Notice that policy 3 is an extremely ill distributed policy. Because the walking action is rarely taken, it requires more samples for on-policy learning to reflect the dynamics of policy 3 than off-policy learning. If one learns policy 3 using samples from itself (on-policy evaluation), the convergence is very slow since it can almost only learn the value functions of states 2,, 8, 6, 4, 2,. This kind of policies does exist in practice. For example, Koller and Parr (2) showed that the uneven distribution of states/actions under a policy can cause problems for approximate policy iteration.</p><p><span class="badge bg-blue-hoki">5</span> .3 stochastic LAM LSTD RMSE(Log) Number of Episodes (Log) Number of Episodes(Log) Figure 2. The Boyan MDP: learning policy 2 with the linear Figure 4. The Boyan MDP: learning policy 3 with tabular stochastic LAM LSTD.5 stochastic LAM LSTD Number of Episodes (Log) Number of Episodes (Log) Figure 3. The Boyan MDP: learning policy 2 with tabular Figure 5. The Boyan MDP: learning policy 3 with the linear In this case, policy 3 is almost the optimal policy, which however is poorly evaluated using on-policy learning. The value function of policy 3 is almost [-8, -7, -5, -4, -2, -, -9, -8, -6, -5, -3, -2, ]. Figure 4 shows that on-policy learning is only able to learn the value functions of states 2,, 8, 6, 4, 2,, because the other states are rarely seen in the episodes. In fact, for on-policy learning the estimation of these value functions remains at the initial guess, which was for the experiment because LSTD s data structures were initialized to. Then the values of the rarely seen states are rarely updated, leading to a rather large error for on-policy learning. The problem is inherent with onpolicy learning. In extreme situations like this, it takes an agent a life time in getting a good estimation of the rarely visited states using on-policy learning. Off-policy learning just doesn t have such a problem. It is not influenced by whatever frequencies that the states/actions are visited/taken by target policies. As long as there are good LAM, evaluation of the tar- get policies is as accurate as the features permit. The quality of LAM is depent on whether the collection policy can collect sufficient samples. Therefore in practice, designing good collection policies is a key issue in learning LAM, which is however beyond the topic of this paper. Figure 4 shows that the two versions of LAM-LSTD both perform very well. Their RMSEs are close because the stochastic LAM-LSTD is almost deterministic. Finally, Figure 5 shows the results of using the linear On-policy learning does a better job than the tabular case simply because of the generalization in the 4.2. Grid-world A grid-world example is shown in Figure 6. There are four actions available in each state. An action moves the agent one grid in the inted direction, except that when leading to the boundary the agent remains in the original state. Reaching the left-up and right-down corners receives a reward.; reaching the</p>                                            <div style="padding-bottom: 15px;" align="center"><script async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/adsbygoogle.js"></script>
                                                <!-- docplayer_3 -->
                                                <ins class="adsbygoogle" style="display: block; height: 90px;" data-ad-client="ca-pub-8693394182295906" data-ad-slot="1884021070" data-ad-format="auto" data-adsbygoogle-status="done"><ins id="aswift_5_expand" style="display:inline-table;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:1034px;background-color:transparent"><ins id="aswift_5_anchor" style="display:block;border:none;height:90px;margin:0;padding:0;position:relative;visibility:visible;width:1034px;background-color:transparent"><iframe width="1034" height="90" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_5" name="aswift_5" style="left:0;position:absolute;top:0;" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/saved_resource(7).html"></iframe></ins></ins></ins>
                                                <script>
                                                    (adsbygoogle = window.adsbygoogle || []).push({});
                                                </script>
                                            </div>
                                        <p><span class="badge bg-blue-hoki">6</span> .4.2 Off-policy Learning with LAM - % S 25% 25% 4% Target policy % 25% 25% 49% Target policy 2 y x.8 - Figure 6. A Grid-world example. The darker is a region, the more the region is covered by the target policies. left-down and right-up corners receives a reward.; and the other rewards are. The task is to evaluate two target policies as shown in the figure. Each run consists of episodes of data up to steps. In each episode, the agent started from the position S in the figure, and behaved according to the targetpolicy (for on-policy learning) or the collection policy (for off-policy learning). For on-policy learning experiments, we used LSTD for evaluating the two target policies. For off-policy learning experiments, the agent followed a purely random policy for collection of samples, and used the deterministic LAM-LSTD for evaluating the projected experience by LAM. Features are tabular for both on-policy and off-policy learning. The on-policy learning result for policy is shown in Figure 7, and the off-policy learning result is shown in Figure 8. The RMS errors of the two learning were compared in Figure 9. The results in all the three figures were averaged over 3 runs. The results are very intuitive. For on-policy learning, the agent was exploring the lower part of the world much more often; the upper part was not well covered. The effect can be clearly seen from the boundary, x = 6. Because the policy goes to the left and right equally often, so along x = 6 the values of the states are (resulting from the fact that the rewards on the left and right of the world have the same magnitude but opposite signs). For the states (x = 6, y 6), their values were learned accurately, reflected in that the -value boundary is almost x = 6. However, for the states (x = 6, y &gt; 6), their values were learned poorly, reflected in that the -value boundary is twisted from x = 6. This, in general, is an inherent problem with on-policy learning, Figure 7. On-policy learning of target policy on Gridworld: the contour of the averaged learned policy. y x Figure 8. Off-policy learning of target policy on Gridworld: the contour of the learned policy (averaged over 3 runs). regardless what algorithms are used. 2 The off-policy learning doesn t have this problem. In Figure 8, the -value boundary is sharply close to x = 6. Also, the learned values of the upper states are much more accurate than those by on-policy learning. This leads to a much smaller RMSE for off-policy learning. The problem of on-policy learning becomes much severe for those policies that rarely visit some states. For the second policy, the agent goes to the lower part almost sure, leaving the learning of the upper states almost a gap. This causes a large learning error for the value function, as shown in Figure A Difficult Control Problem We studied the bicycle-riding-balancing task, which is considered as a difficult problem in literature (Lagoudakis &amp; Parr, 23). The state variable is (ϑ, ϑ, ω, ω, ω, ψ, d, x b, y b, x f, y f ), where θ is the angle of the handler (abusing notation from the weight vector), ω is the vertical angle of the bicycle, d is the 2 Function approximation may help this problem, but this deps on if the chosen features can generalize appropriately to those regions not covered..6.8</p><p><span class="badge bg-blue-hoki">7</span> 2 3 4 Off policy, policy Off policy, policy 2 On policy, policy 2 On policy, policy 2 Number of episodes (Log) Figure 9. The averaged RMSE of on/off-policy learning on Grid-world. distance to goal, ψ is the angle of the bicycle to the goal, and (x b, y b )/(x f, y f ) is the back/front tyre position. The actions are the torque applied to the handler, τ { 2,, 2}; and the displacement of the rider, v {.2,,.2}. At least one of τ and v is restricted to. This leads to 5 actions in total. If ω is bigger than π/5, the bicycle falls over and the episode stops. The reward signal is updated according to r t = 5ω t π 2 5ω t π 2 + d t d t, where d t is the distance from the bicycle to the goal at the t time step. The discount factor is.8. The state feature is the same as used for a single action in LSPI, comprising 2 basis functions: [, ω, ω, ω 2, ω 2, ω ω, θ, θ, θ 2, θ 2, θ θ, ωθ, ωθ 2, ω 2 θ, ψ, ψ 2, ψθ, ψ, ψ 2, ψθ] T, where ψ = π ψ if ψ &gt;, otherwise ψ = π ψ. The problem is difficult partially because there is a noise added to the displacement action, which follows a uniformly distribution in [.2,.2]. We collected a data set of 25 episodes using a uniformly random policy, each comprising 2 steps of samples. Five linear action models were learned using the least-squares algorithm. We used LAM to evaluate the following three policies: () policy, which takes the five actions with probabilities.4,,.4,.,.; (2) policy 2 is coined. With probability., select the action that minimizes the predicted direction to goal (ψ), riding to the goal; with probability.9, select the action that minimizes the predicted vertical angle of the bicycle (ω), balancing. The predictions are made according to LAM. For a feature φ, we have five feature projections, φ a = F a φ. For each a, φa (2) gives the predicted ω and φ a (5) gives the predicted ψ after taking the action; (3) policy 3 is the greedy policy. At a feature φ in the samples, we select a = arg max a { Q(φ, a) = arg max φ T f a + γ(f a φ) T θ }, a () (where θ is the policy weight vector). Then the projected experience, (φ, a, r = φ T f a, φ = F a φ) is fed to LSTD. Policies and 2 can be evaluated using the deterministic or stochastic LAM-LSTD. We added an outer iteration loop for learning policy 3 for convergence (in a few iterations). After the value functions (or parameters θ) of the three policies were learned, we used them for control, selecting actions online according to (), in which φ now takes real time Figure shows the trajectories of the three controllers. Notice that policy is a poor one, according to which the bicycle fell over in less than one hundred of steps most of the time. Surprisingly, action selection according to the value function of policy (i.e., θ ) in the way of equation () can balance the bicycle for at least 72, steps, as shown in the figure. This is because the value function of policy approximates the shape of the balancing policy well, though the policy itself is poor. Figure shows the learned value function of policy : V (s) = φ(s) T θ, where s (only the ω is shown) takes the states in episodes of the samples. For most trajectories, V increases as ω reduces. Thus action selection through maximizing V has the effect of balancing the bicycle. That on the few trajectories where V does not increase as ω decreases is because the value function also deps on other factors such as the angle to the goal. Figure 2 and Figure 3 show the value function of policy 2 versus the sampled back tyre positions of the bicycle on the domain. The left and right plots used the same colors and markers for values of the same states from samples, so there is a correspondence between the two plots. For example, the largest values, on the top of both plots, corresponds to two episodes of states whose x b is non-negative and y b close to. Figure 2 shows that V 2 generally increases as x b increases. Figure 3 shows V 2 generally increases as y b decreases. 3 Thus action selection through maximizing V 2 has the effect of pushing the bicycle along the positive direction in the x-axis. The balancing aspect of policy 2 is similar to policy, and thus not shown. The shape of V 3 is similar to V 2, and thus omitted as well. Off-policy learning on complex control problems is exclusively important because on-policy learning is of- 3 Again there are exceptions for some episodes because of the depence on other factors.</p><p><span class="badge bg-blue-hoki">8</span> 3.5 x 3 2 yb 2 policy 3 policy 2 Value Function V policy xb xb Figure. The Bicycle domain: trajectories of acting through maximizing the value functions of the three policies. The value functions were learned using LAM-LSTD off-policy learning. Figure 2. The Bicycle: the learned value function for policy 2 versus x b..5 x 3 Value Function V.5 x omega Figure. The Bicycle domain: the value function of policy versus ω, shown on episodes of samples. Notice maximizing the value function has the effect of reducing ω most of the time. ten difficult. For instance, in this example behaving according to policy mostly fell under episodes. Short episodes of samples under target policies can be very common since the policies can fail long before the goal of an agent is reached. In cases where important rewards are given upon reaching the goal, which are the most common in reinforcement learning, on-policy learning of many policies could not collect sufficient good samples. 4 Moreover, on-policy evaluation requires as many sets of samples as the policies, for which off-policy learning can use only one set of samples off-policy learning is just more data efficient. 4 This does not create a problem for this example since important rewards are given not upon reaching the goal but upon driving along the direction to it. Value Function V yb Figure 3. The Bicycle: the learned value function for policy 2 versus y b. Action selection through maximizing V 2 has the effect of pushing to the positive direction in the x-axis. 5. Discussion and Conclusion Off-policy learning is an interesting topic, which has been pursued since the early days of RL. The goal of off-policy learning is very charming: using a single stream of data collected from an arbitrary policy to evaluate any other policy. Researchers have proposed importance sampling (Precup et al., 2) and gradient descent methods (Sutton et al., 29) to this goal. These methods are generally model-free. We proposed a model-based method for efficient offpolicy learning. Given a data set of samples, we first learn a set of linear action models. The linear action models are then used to project the experience under a target policy. Off-policy learning algorithms such as LSTD and GTD can then be applied to evaluating the projected experience. We proposed two off-policy learning algorithms with LAM, based on two ways of projecting experience. Empirical results of evaluating various policies show that our algorithms performed</p><p><span class="badge bg-blue-hoki">9</span> very well. Our results suggest that off-policy learning is a promising way of improving the efficiency of using samples. As long as collection of data is allowed on a problem, off-policy learning can replace on-policy learning in evaluating various policies. For RL problems where interacting with the environment is time consuming or money expensive, our method provides a very cheap solution, a one-collection-for-all solution: one time of data collection from interaction with the environment can provide accurate evaluation of as many policies as a RL researcher is interested in. We noticed that the linear Gaussian MDP model (Bowling et al., 28) is also action-depent but policy-indepent. The linear MDP model is learned from samples, and then used to explicitly construct the policy models for approximate policy iteration using sigma-points methods. As noted by Bowling et. al. (28), this method can get rid of samples after the model is learned and is computationally faster than LSPI which memorizes and sweeps the samples at each iteration. This observation also holds for an extension of our method to approximate policy iteration (see (Yao, 2)). The major difference of our method is that we do not construct the policy models explicitly, but use the LAM to project samples for different policies, which is more efficient in both computation and memory. Though the model-based property is the uniqueness of our approach to off-policy learning, we didn t go into comparing with the model-free approach. The model-based approach is generally known to be more data efficient, but more complex in per-time-step complexity (Moore &amp; Atkeson, 993; Kaebling et al., 996; Sutton &amp; Barto, 998; Sutton et al., 28). For example, LSTD produces more accurate predictions than TD, but its per-time-step computational complexity is higher than TD (Bradtke &amp; Barto, 996; Boyan, 22; Xu et al., 22). These conclusions still hold for the comparisons between our model-based algorithms and model-free algorithms of off-policy learning. Our solution is more data efficient, but is O(n 2 ) per time step in computation, for which gradient TD is O(n). Furthermore, our model-based approach to off-policy learning does not exclude the use of modelfree off-policy learning algorithms. For example, GTD algorithms can be used to evaluate the projected experience by LAM, giving several LAM-GTD algorithms. Acknowledgement I gratefully give thanks to Csaba Szepesvári, Rich Sutton, Joseph Modayil, Yasin Abbasi-Yadkori, István Szita, Amir massoud Farahmand, Mike Bowling, Lihong Li, and Eric Hansen for many helpful discussions. I also specially thank Michail Lagoudakis for sing me the bicycle simulator. References Bowling, Michael, Geramifard, Alborz, and Wingate, David. Sigma point policy iteration. In AAMAS, pp , 28. Boyan, J. A. Technical update: Least-squares temporal difference learning. Machine Learning, 49: , 22. Bradtke, S. and Barto, A. G. Linear least-squares algorithms for temporal difference learning. Machine Learning, 22:33 57, 996. Frank, J., Mannor, S., and Precup, D. Reinforcement learning in the presence of rare events. In ICML, 28. Kaebling, L.P., Littman, M.L., and Moore, A.W. Reinforcement learning: A survey. JAIR, 4: , 996. Koller, D. and Parr, R. Policy iteration for factored MDPs. In UAI, 2. Lagoudakis, M. and Parr, R. Least-squares policy iteration. JMLR, 4:7 49, 23. Moore, A. W. and Atkeson, C. G. Prioritized sweeping: Reinforcement learning with less data and less time. Machine Learning, 3():3 3, 993. Parr, R., Li, L., Taylor, G., Painter-Wakefiled, C., and Littman, M. L. An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning. In ICML, 28. Precup, Doina, Sutton, Richard S., and Dasgupta, Sanjoy. Off-policy temporal-difference learning with function approximation. In ICML, 2. Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. MIT Press, 998. Sutton, R. S., Szepesvári, Cs., Geramifard, A., and Bowling, M. Dyna-style planning with linear function approximation and prioritized sweeping. In UAI, 28. Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesvari, Cs., and Wiewiora, E. Fast gradientdescent methods for temporal-difference learning with linear function approximation. In ICML, 29. Watkins, C. J. C. H. Learning from delayed rewards. PhD thesis, University of Cambridge, England, 989. Xu, X., He, H., and Hu, D. Efficient reinforcement learning using recursive least-squares methods. JAIR, 6: , 22. Yao, H. Approximate policy iteration with linear action models. Technical Report TR -7, Department of Computing Science, University of Alberta, 2.</p>                                    </div>
                                </div>
                                <!-- END LEFT SIDEBAR -->
                            </div>



                            <!-- BEGIN RIGHT SIDEBAR -->
                            <div class="col-md-3 news-page col-sm-12 pull-right">
                                                                    <div style="margin-top: 5px; margin-bottom: 10px;">
                                        <script>
                                            document.write('<sc'+'ript async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></sc'+'ript> \
                        <ins class="adsbygoogle"\
                             style="display:block"\
                             data-ad-client="ca-pub-8693394182295906"\
                             data-ad-slot="8753203876"\
                             data-ad-format="auto"></ins>\
                        <scr'+'ipt>\
                            (adsbygoogle = window.adsbygoogle || []).push({});\
                        </'+'scr'+'ipt>');
                                        </script><script async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/adsbygoogle.js"></script>                         <ins class="adsbygoogle" style="display: block; height: 50px;" data-ad-client="ca-pub-8693394182295906" data-ad-slot="8753203876" data-ad-format="auto" data-adsbygoogle-status="done"><ins id="aswift_6_expand" style="display:inline-table;border:none;height:50px;margin:0;padding:0;position:relative;visibility:visible;width:325px;background-color:transparent"><ins id="aswift_6_anchor" style="display:block;border:none;height:50px;margin:0;padding:0;position:relative;visibility:visible;width:325px;background-color:transparent"><iframe width="325" height="50" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_6" name="aswift_6" style="left:0;position:absolute;top:0;" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/saved_resource(8).html"></iframe></ins></ins></ins>                        <script>                            (adsbygoogle = window.adsbygoogle || []).push({});                        </script>
                                    </div>
                                

                                <div class="top-news">
                                    <a href="http://docplayer.net/37218908-Off-policy-learning-with-linear-action-models-an-efficient-one-collection-for-all-solution.html#" class="btn purple">
                                        <span>Similar documents</span>
                                        <i class="fa fa-puzzle-piece top-news-icon"></i>
                                    </a>
                                </div>
                                <div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34606159-B553-lecture-4-gradient-descent.html">B553 Lecture 4: Gradient Descent</a>
    </h3>
    <p>
        <img alt="B553 Lecture 4: Gradient Descent" title="B553 Lecture 4: Gradient Descent" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34606159.jpg">
        B553 Lecture 4: Gradient Descent Kris Hauser January 24, 2012 The first multivariate optimization technique we will examine is one of the simplest: gradient descent (also known as steepest descent). Gradient    </p>
    <a href="http://docplayer.net/34606159-B553-lecture-4-gradient-descent.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34128332-Cs-229-public-course-problem-set-4-unsupervised-learning-and-reinforcement.html">CS 229, Public Course Problem Set #4: Unsupervised Learning and Reinforcement</a>
    </h3>
    <p>
        <img alt="CS 229, Public Course Problem Set #4: Unsupervised Learning and Reinforcement" title="CS 229, Public Course Problem Set #4: Unsupervised Learning and Reinforcement" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34128332.jpg">
        CS229 Problem Set #4 1 CS 229, Public Course Problem Set #4: Unsupervised Learning and Reinforcement Learning 1. EM for supervised learning In class we applied EM to the unsupervised learning setting.    </p>
    <a href="http://docplayer.net/34128332-Cs-229-public-course-problem-set-4-unsupervised-learning-and-reinforcement.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34965390-A-vision-for-reinforcement-learning-and-predictive-maintenance.html">A vision for reinforcement learning and predictive maintenance</a>
    </h3>
    <p>
        <img alt="A vision for reinforcement learning and predictive maintenance" title="A vision for reinforcement learning and predictive maintenance" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34965390.jpg">
        A vision for reinforcement learning and predictive maintenance Charles Elkan University of California, San Diego August 21, 2011 1 / 29 What is the goal of maintenance? Preventive maintenance is a small    </p>
    <a href="http://docplayer.net/34965390-A-vision-for-reinforcement-learning-and-predictive-maintenance.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/17589064-Beyond-reward-the-problem-of-knowledge-and-data.html">Beyond Reward: The Problem of Knowledge and Data</a>
    </h3>
    <p>
        <img alt="Beyond Reward: The Problem of Knowledge and Data" title="Beyond Reward: The Problem of Knowledge and Data" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/17589064.jpg">
        Beyond Reward: The Problem of Knowledge and Data Richard S. Sutton University of Alberta Edmonton, Alberta, Canada Intelligence can be defined, informally, as knowing a lot and being able to use that knowledge    </p>
    <a href="http://docplayer.net/17589064-Beyond-reward-the-problem-of-knowledge-and-data.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
                                        <div class="top-news">
                                            <script async="" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/adsbygoogle.js"></script>
                                            <!-- adaptive5 -->
                                            <ins class="adsbygoogle" style="display: block; height: 50px;" data-ad-client="ca-pub-8693394182295906" data-ad-slot="7056144675" data-ad-format="auto" data-adsbygoogle-status="done"><ins id="aswift_7_expand" style="display:inline-table;border:none;height:50px;margin:0;padding:0;position:relative;visibility:visible;width:325px;background-color:transparent"><ins id="aswift_7_anchor" style="display:block;border:none;height:50px;margin:0;padding:0;position:relative;visibility:visible;width:325px;background-color:transparent"><iframe width="325" height="50" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_7" name="aswift_7" style="left:0;position:absolute;top:0;" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/saved_resource(9).html"></iframe></ins></ins></ins>
                                            <script>
                                                (adsbygoogle = window.adsbygoogle || []).push({});
                                            </script>
                                        </div>
                                    <div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33030344-Solving-the-bellman-optimality-equations-basic-methods.html">Solving the Bellman Optimality Equations: Basic Methods</a>
    </h3>
    <p>
        <img alt="Solving the Bellman Optimality Equations: Basic Methods" title="Solving the Bellman Optimality Equations: Basic Methods" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33030344.jpg">
        Solving the Bellman Optimality Equations: Basic Methods AI &amp; Agents for IET Lecturer: S Luz http://www.scss.tcd.ie/~luzs/t/cs7032/ December 3, 2014 The basic background &amp; assumptions Environment is a finite    </p>
    <a href="http://docplayer.net/33030344-Solving-the-bellman-optimality-equations-basic-methods.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/32972036-Least-squares-policy-iteration.html">Least-Squares Policy Iteration</a>
    </h3>
    <p>
        <img alt="Least-Squares Policy Iteration" title="Least-Squares Policy Iteration" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/32972036.jpg">
        Journal of Machine Learning Research (3) 7-9 Submitted 8/; Published /3 Least-Squares Policy Iteration Michail G. Lagoudakis Ronald Parr Department of Computer Science Duke University Durham, NC 778, USA    </p>
    <a href="http://docplayer.net/32972036-Least-squares-policy-iteration.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/31447883-The-single-neuron-as-a-classifier.html">The Single Neuron as a Classifier</a>
    </h3>
    <p>
        <img alt="The Single Neuron as a Classifier" title="The Single Neuron as a Classifier" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/31447883.jpg">
        Copyright Cambridge University Press. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/9 You can buy this book for pounds or $. See http://www.inference.phy.cam.ac.uk/mackay/itila/    </p>
    <a href="http://docplayer.net/31447883-The-single-neuron-as-a-classifier.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/19301397-A-sarsa-based-autonomous-stock-trading-agent.html">A Sarsa based Autonomous Stock Trading Agent</a>
    </h3>
    <p>
        <img alt="A Sarsa based Autonomous Stock Trading Agent" title="A Sarsa based Autonomous Stock Trading Agent" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/19301397.jpg">
        A Sarsa based Autonomous Stock Trading Agent Achal Augustine The University of Texas at Austin Department of Computer Science Austin, TX 78712 USA achal@cs.utexas.edu Abstract This paper describes an autonomous    </p>
    <a href="http://docplayer.net/19301397-A-sarsa-based-autonomous-stock-trading-agent.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/12169121-Eligibility-traces-suggested-reading-contents-chapter-7-in-r-s-sutton-a-g-barto-reinforcement-learning-an-introduction-mit-press-1998.html">Eligibility Traces. Suggested reading: Contents: Chapter 7 in R. S. Sutton, A. G. Barto: Reinforcement Learning: An Introduction MIT Press, 1998.</a>
    </h3>
    <p>
        <img alt="Eligibility Traces. Suggested reading: Contents: Chapter 7 in R. S. Sutton, A. G. Barto: Reinforcement Learning: An Introduction MIT Press, 1998." title="Eligibility Traces. Suggested reading: Contents: Chapter 7 in R. S. Sutton, A. G. Barto: Reinforcement Learning: An Introduction MIT Press, 1998." class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/12169121.jpg">
        Eligibility Traces 0 Eligibility Traces Suggested reading: Chapter 7 in R. S. Sutton, A. G. Barto: Reinforcement Learning: An Introduction MIT Press, 1998. Eligibility Traces Eligibility Traces 1 Contents:    </p>
    <a href="http://docplayer.net/12169121-Eligibility-traces-suggested-reading-contents-chapter-7-in-r-s-sutton-a-g-barto-reinforcement-learning-an-introduction-mit-press-1998.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/31732780-Framework-of-automatic-text-summarization-using-reinforcement-learning.html">Framework of Automatic Text Summarization Using Reinforcement Learning</a>
    </h3>
    <p>
        <img alt="Framework of Automatic Text Summarization Using Reinforcement Learning" title="Framework of Automatic Text Summarization Using Reinforcement Learning" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/31732780.jpg">
        Framework of Automatic Text Summarization Using Reinforcement Learning Seonggi Ryang Graduate School of Information Science and Technology University of Tokyo sryang@is.s.u-tokyo.ac.jp Takeshi Abekawa    </p>
    <a href="http://docplayer.net/31732780-Framework-of-automatic-text-summarization-using-reinforcement-learning.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33789037-Kalman-temporal-differences.html">Kalman Temporal Differences</a>
    </h3>
    <p>
        <img alt="Kalman Temporal Differences" title="Kalman Temporal Differences" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33789037.jpg">
        Journal of Artificial Intelligence Research 39 (2010) 483-532 Submitted 04/10; published 10/10 Kalman Temporal Differences Matthieu Geist Olivier Pietquin IMS research group Supélec Metz, France matthieu.geist@supelec.fr    </p>
    <a href="http://docplayer.net/33789037-Kalman-temporal-differences.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33867143-Markov-decision-processes-and-exact-solution-methods-value-iteration-policy-iteration-linear-programming.html">Markov Decision Processes and Exact Solution Methods: Value Iteration Policy Iteration Linear Programming</a>
    </h3>
    <p>
        <img alt="Markov Decision Processes and Exact Solution Methods: Value Iteration Policy Iteration Linear Programming" title="Markov Decision Processes and Exact Solution Methods: Value Iteration Policy Iteration Linear Programming" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33867143.jpg">
        Markov Decision Processes and Exact Solution Methods: Value Iteration Policy Iteration Linear Programming Pieter Abbeel UC Berkeley EECS Markov Decision Process Assumption: agent gets to observe the state    </p>
    <a href="http://docplayer.net/33867143-Markov-decision-processes-and-exact-solution-methods-value-iteration-policy-iteration-linear-programming.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/24083928-Tetris-experiments-with-the-lp-approach-to-approximate-dp.html">Tetris: Experiments with the LP Approach to Approximate DP</a>
    </h3>
    <p>
        <img alt="Tetris: Experiments with the LP Approach to Approximate DP" title="Tetris: Experiments with the LP Approach to Approximate DP" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/24083928.jpg">
        Tetris: Experiments with the LP Approach to Approximate DP Vivek F. Farias Electrical Engineering Stanford University Stanford, CA 94403 vivekf@stanford.edu Benjamin Van Roy Management Science and Engineering    </p>
    <a href="http://docplayer.net/24083928-Tetris-experiments-with-the-lp-approach-to-approximate-dp.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/21565057-Learning-tetris-using-the-noisy-cross-entropy-method.html">Learning Tetris Using the Noisy Cross-Entropy Method</a>
    </h3>
    <p>
        <img alt="Learning Tetris Using the Noisy Cross-Entropy Method" title="Learning Tetris Using the Noisy Cross-Entropy Method" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/21565057.jpg">
        NOTE Communicated by Andrew Barto Learning Tetris Using the Noisy Cross-Entropy Method István Szita szityu@eotvos.elte.hu András Lo rincz andras.lorincz@elte.hu Department of Information Systems, Eötvös    </p>
    <a href="http://docplayer.net/21565057-Learning-tetris-using-the-noisy-cross-entropy-method.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34607365-Fast-gradient-descent-methods-for-temporal-difference-learning-with-linear-function-approximation.html">Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation</a>
    </h3>
    <p>
        <img alt="Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation" title="Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34607365.jpg">
        Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation Richard S. Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesvári, Eric    </p>
    <a href="http://docplayer.net/34607365-Fast-gradient-descent-methods-for-temporal-difference-learning-with-linear-function-approximation.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33833825-Math-56-compu-expt-math-spring-2014-homework-1.html">Math 56 Compu &amp; Expt Math, Spring 2014: Homework 1</a>
    </h3>
    <p>
        <img alt="Math 56 Compu &amp; Expt Math, Spring 2014: Homework 1" title="Math 56 Compu &amp; Expt Math, Spring 2014: Homework 1" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33833825.jpg">
        Math 56 Compu &amp; Expt Math, Spring 2014: Homework 1 Pawan Dhakal April 9, 2014 1. Asymptotics. e n (a) Is 10 + ne n = O(n 1 ) as n? Prove your answer, i.e. if true, exhibit a C and n 0 in the definition    </p>
    <a href="http://docplayer.net/33833825-Math-56-compu-expt-math-spring-2014-homework-1.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/20685450-Motivation-motivation-can-a-software-agent-learn-to-play-backgammon-by-itself-machine-learning-reinforcement-learning.html">Motivation. Motivation. Can a software agent learn to play Backgammon by itself? Machine Learning. Reinforcement Learning</a>
    </h3>
    <p>
        <img alt="Motivation. Motivation. Can a software agent learn to play Backgammon by itself? Machine Learning. Reinforcement Learning" title="Motivation. Motivation. Can a software agent learn to play Backgammon by itself? Machine Learning. Reinforcement Learning" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/20685450.jpg">
        Motivation Machine Learning Can a software agent learn to play Backgammon by itself? Reinforcement Learning Prof. Dr. Martin Riedmiller AG Maschinelles Lernen und Natürlichsprachliche Systeme Institut    </p>
    <a href="http://docplayer.net/20685450-Motivation-motivation-can-a-software-agent-learn-to-play-backgammon-by-itself-machine-learning-reinforcement-learning.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/30812668-Markov-chains-and-markov-random-fields-mrfs.html">Markov chains and Markov Random Fields (MRFs)</a>
    </h3>
    <p>
        <img alt="Markov chains and Markov Random Fields (MRFs)" title="Markov chains and Markov Random Fields (MRFs)" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/30812668.jpg">
        Markov chains and Markov Random Fields (MRFs) 1 Why Markov Models We discuss Markov models now. This is the simplest statistical model in which we don t assume that all variables are independent; we assume    </p>
    <a href="http://docplayer.net/30812668-Markov-chains-and-markov-random-fields-mrfs.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/25885184-Rational-and-convergent-learning-in-stochastic-games.html">Rational and Convergent Learning in Stochastic Games</a>
    </h3>
    <p>
        <img alt="Rational and Convergent Learning in Stochastic Games" title="Rational and Convergent Learning in Stochastic Games" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/25885184.jpg">
        + ' ' Rational and Convergent Learning in Stochastic Games Michael Bowling mhb@cs.cmu.edu Manuela Veloso veloso@cs.cmu.edu Computer Science Department Carnegie Mellon University Pittsburgh, PA 523-389    </p>
    <a href="http://docplayer.net/25885184-Rational-and-convergent-learning-in-stochastic-games.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/24215397-Advantage-updating-leemon-c-baird-iii-technical-report-wl-tr-wright-laboratory-wright-patterson-air-force-base-oh.html">ADVANTAGE UPDATING. Leemon C. Baird III. Technical Report WL-TR Wright Laboratory. Wright-Patterson Air Force Base, OH</a>
    </h3>
    <p>
        <img alt="ADVANTAGE UPDATING. Leemon C. Baird III. Technical Report WL-TR Wright Laboratory. Wright-Patterson Air Force Base, OH" title="ADVANTAGE UPDATING. Leemon C. Baird III. Technical Report WL-TR Wright Laboratory. Wright-Patterson Air Force Base, OH" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/24215397.jpg">
        ADVANTAGE UPDATING Leemon C. Baird III Technical Report WL-TR-93-1146 Wright Laboratory Wright-Patterson Air Force Base, OH 45433-7301 Address: WL/AAAT, Bldg 635 2185 Avionics Circle Wright-Patterson Air    </p>
    <a href="http://docplayer.net/24215397-Advantage-updating-leemon-c-baird-iii-technical-report-wl-tr-wright-laboratory-wright-patterson-air-force-base-oh.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/29410123-Log-optimal-investment-in-markovian-environments.html">Log-optimal Investment in Markovian Environments</a>
    </h3>
    <p>
        <img alt="Log-optimal Investment in Markovian Environments" title="Log-optimal Investment in Markovian Environments" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/29410123.jpg">
        Log-optimal Investment in Markovian Environments Computer and Automation Research Institute of the Hungarian Academy of Sciences Kende u. 13-17, Budapest 1111, Hungary E-mail: szcsaba@sztaki.hu Morgen    </p>
    <a href="http://docplayer.net/29410123-Log-optimal-investment-in-markovian-environments.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34084009-Scaling-life-long-off-policy-learning.html">Scaling Life-long Off-policy Learning</a>
    </h3>
    <p>
        <img alt="Scaling Life-long Off-policy Learning" title="Scaling Life-long Off-policy Learning" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34084009.jpg">
        Scaling Life-long Off-policy Learning Adam White, Joseph Modayil and Richard S. Sutton Reinforcement Learning and Artificial Intelligence Laboratory Department of Computing Science University of Alberta,    </p>
    <a href="http://docplayer.net/34084009-Scaling-life-long-off-policy-learning.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/32620883-Markov-decision-processes-machine-learning-fall-2012-recitation-november-27-2012-selen-uguroglu.html">Markov Decision Processes Machine Learning Fall 2012 Recitation November 27, 2012 Selen Uguroglu</a>
    </h3>
    <p>
        <img alt="Markov Decision Processes Machine Learning Fall 2012 Recitation November 27, 2012 Selen Uguroglu" title="Markov Decision Processes Machine Learning Fall 2012 Recitation November 27, 2012 Selen Uguroglu" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/32620883.jpg">
        Markov Decision Processes 10-601 Machine Learning Fall 2012 Recitation November 27, 2012 Selen Uguroglu 1 Outline 1. MDP overview 2. Value Iteration 3. Policy Iteration 4. Previous Exam Questions 2 Outline    </p>
    <a href="http://docplayer.net/32620883-Markov-decision-processes-machine-learning-fall-2012-recitation-november-27-2012-selen-uguroglu.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/30813445-Markov-chains-part-i.html">Markov Chains, part I</a>
    </h3>
    <p>
        <img alt="Markov Chains, part I" title="Markov Chains, part I" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/30813445.jpg">
        Markov Chains, part I December 8, 2010 1 Introduction A Markov Chain is a sequence of random variables X 0, X 1,, where each X i S, such that P(X i+1 = s i+1 X i = s i, X i 1 = s i 1,, X 0 = s 0 ) = P(X    </p>
    <a href="http://docplayer.net/30813445-Markov-chains-part-i.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/29901219-Computational-intelligence-introduction-to-machine-learning-ss16-lecture-2-linear-regression-gradient-descent-non-linear-basis-functions.html">COMPUTATIONAL INTELLIGENCE (INTRODUCTION TO MACHINE LEARNING) SS16. Lecture 2: Linear Regression Gradient Descent Non-linear basis functions</a>
    </h3>
    <p>
        <img alt="COMPUTATIONAL INTELLIGENCE (INTRODUCTION TO MACHINE LEARNING) SS16. Lecture 2: Linear Regression Gradient Descent Non-linear basis functions" title="COMPUTATIONAL INTELLIGENCE (INTRODUCTION TO MACHINE LEARNING) SS16. Lecture 2: Linear Regression Gradient Descent Non-linear basis functions" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/29901219.jpg">
        COMPUTATIONAL INTELLIGENCE (INTRODUCTION TO MACHINE LEARNING) SS16 Lecture 2: Linear Regression Gradient Descent Non-linear basis functions LINEAR REGRESSION MOTIVATION Why Linear Regression? Regression    </p>
    <a href="http://docplayer.net/29901219-Computational-intelligence-introduction-to-machine-learning-ss16-lecture-2-linear-regression-gradient-descent-non-linear-basis-functions.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/21861169-The-effects-of-start-prices-on-the-performance-of-the-certainty-equivalent-pricing-policy.html">The Effects of Start Prices on the Performance of the Certainty Equivalent Pricing Policy</a>
    </h3>
    <p>
        <img alt="The Effects of Start Prices on the Performance of the Certainty Equivalent Pricing Policy" title="The Effects of Start Prices on the Performance of the Certainty Equivalent Pricing Policy" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/21861169.jpg">
        BMI Paper The Effects of Start Prices on the Performance of the Certainty Equivalent Pricing Policy Faculty of Sciences VU University Amsterdam De Boelelaan 1081 1081 HV Amsterdam Netherlands Author: R.D.R.    </p>
    <a href="http://docplayer.net/21861169-The-effects-of-start-prices-on-the-performance-of-the-certainty-equivalent-pricing-policy.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/24083917-Learning-tetris-using-the-noisy-cross-entropy-method.html">Learning Tetris Using the Noisy Cross-Entropy Method</a>
    </h3>
    <p>
        <img alt="Learning Tetris Using the Noisy Cross-Entropy Method" title="Learning Tetris Using the Noisy Cross-Entropy Method" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/24083917.jpg">
        Learning Tetris Using the Noisy Cross-Entropy Method István Szita 1, András Lőrincz 2 Department of Information Systems, Eötvös Loránd University Pázmány Péter sétány 1/C, Budapest, Hungary H-1117 Abstract    </p>
    <a href="http://docplayer.net/24083917-Learning-tetris-using-the-noisy-cross-entropy-method.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33719606-Random-projections-for-anchor-based-topic-inference.html">Random Projections for Anchor-based Topic Inference</a>
    </h3>
    <p>
        <img alt="Random Projections for Anchor-based Topic Inference" title="Random Projections for Anchor-based Topic Inference" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33719606.jpg">
        Random Projections for Anchor-based Topic Inference David Mimno Department of Information Science Cornell University Ithaca, NY 14850 mimno@cornell.edu Abstract Recent spectral topic discovery methods    </p>
    <a href="http://docplayer.net/33719606-Random-projections-for-anchor-based-topic-inference.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35279226-Deep-reinforcement-learning-david-silver-google-deepmind.html">Deep Reinforcement Learning. David Silver, Google DeepMind</a>
    </h3>
    <p>
        <img alt="Deep Reinforcement Learning. David Silver, Google DeepMind" title="Deep Reinforcement Learning. David Silver, Google DeepMind" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35279226.jpg">
        Deep Reinforcement Learning David Silver, Google DeepMind Reinforcement Learning: AI = RL RL is a general-purpose framework for artificial intelligence RL is for an agent with the capacity to act Each    </p>
    <a href="http://docplayer.net/35279226-Deep-reinforcement-learning-david-silver-google-deepmind.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33831653-Root-finding-methods.html">Root-Finding Methods</a>
    </h3>
    <p>
        <img alt="Root-Finding Methods" title="Root-Finding Methods" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33831653.jpg">
        LECTURE NOTES ECO 613/614 FALL 2007 KAREN A. KOPECKY Root-Finding Methods Often we are interested in finding x such that f(x) = 0, where f : R n R n denotes a system of n nonlinear equations and x is the    </p>
    <a href="http://docplayer.net/33831653-Root-finding-methods.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35554534-Dynamic-systems-and-control-final-project.html">Dynamic Systems and Control Final Project</a>
    </h3>
    <p>
        <img alt="Dynamic Systems and Control Final Project" title="Dynamic Systems and Control Final Project" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35554534.jpg">
        Dynamic Systems and Control Final Project Due Date: September 1st, 2015 1 LQG 1. In a fully observable, deterministic, discrete-time linear dynamical system with dynamics x t+1 = Ax t + Bu t, suppose that    </p>
    <a href="http://docplayer.net/35554534-Dynamic-systems-and-control-final-project.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/31612763-Analytical-mean-squared-error-curves-abstract-we-have-calculated-analytical-expressions-for-how-the-bias-and.html">Analytical Mean Squared Error Curves. Abstract. We have calculated analytical expressions for how the bias and</a>
    </h3>
    <p>
        <img alt="Analytical Mean Squared Error Curves. Abstract. We have calculated analytical expressions for how the bias and" title="Analytical Mean Squared Error Curves. Abstract. We have calculated analytical expressions for how the bias and" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/31612763.jpg">
        Analytical Mean Squared Error Curves in Temporal Dierence Learning Satinder Singh Department of Computer Science University of Colorado Boulder, CO 80309-0430 baveja@cs.colorado.edu Peter Dayan Brain and    </p>
    <a href="http://docplayer.net/31612763-Analytical-mean-squared-error-curves-abstract-we-have-calculated-analytical-expressions-for-how-the-bias-and.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/22422666-Introduction-to-neural-networks.html">INTRODUCTION TO NEURAL NETWORKS</a>
    </h3>
    <p>
        <img alt="INTRODUCTION TO NEURAL NETWORKS" title="INTRODUCTION TO NEURAL NETWORKS" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/22422666.jpg">
        INTRODUCTION TO NEURAL NETWORKS Pictures are taken from http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html http://research.microsoft.com/~cmbishop/prml/index.htm By Nobel Khandaker Neural Networks An    </p>
    <a href="http://docplayer.net/22422666-Introduction-to-neural-networks.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/20804300-Neural-networks-and-reinforcement-learning-abhijit-gosavi.html">NEURAL NETWORKS AND REINFORCEMENT LEARNING. Abhijit Gosavi</a>
    </h3>
    <p>
        <img alt="NEURAL NETWORKS AND REINFORCEMENT LEARNING. Abhijit Gosavi" title="NEURAL NETWORKS AND REINFORCEMENT LEARNING. Abhijit Gosavi" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/20804300.jpg">
        NEURAL NETWORKS AND REINFORCEMENT LEARNING Abhijit Gosavi Department of Engineering Management and Systems Engineering Missouri University of Science and Technology Rolla, MO 65409 1 Outline A Quick Introduction    </p>
    <a href="http://docplayer.net/20804300-Neural-networks-and-reinforcement-learning-abhijit-gosavi.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/26911038-Gradient-methods-rafael-e-banchs.html">Gradient Methods. Rafael E. Banchs</a>
    </h3>
    <p>
        <img alt="Gradient Methods. Rafael E. Banchs" title="Gradient Methods. Rafael E. Banchs" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/26911038.jpg">
        Gradient Methods Rafael E. Banchs INTRODUCTION This report discuss one class of the local search algorithms to be used in the inverse modeling of the time harmonic field electric logging problem, the Gradient    </p>
    <a href="http://docplayer.net/26911038-Gradient-methods-rafael-e-banchs.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/21687170-Reinforcement-learning.html">Reinforcement Learning</a>
    </h3>
    <p>
        <img alt="Reinforcement Learning" title="Reinforcement Learning" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/21687170.jpg">
        Reinforcement Learning LU 2 - Markov Decision Problems and Dynamic Programming Dr. Martin Lauer AG Maschinelles Lernen und Natürlichsprachliche Systeme Albert-Ludwigs-Universität Freiburg martin.lauer@kit.edu    </p>
    <a href="http://docplayer.net/21687170-Reinforcement-learning.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/30114473-Cs181-lecture-5-reinforcement-learning.html">CS181 Lecture 5 Reinforcement Learning</a>
    </h3>
    <p>
        <img alt="CS181 Lecture 5 Reinforcement Learning" title="CS181 Lecture 5 Reinforcement Learning" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/30114473.jpg">
        CS181 Lecture 5 Reinforcement Learning The topic for today is reinforcement learning. We will look at several methods, and then discuss two important issues: the exploration-exploitation tradeoff and the    </p>
    <a href="http://docplayer.net/30114473-Cs181-lecture-5-reinforcement-learning.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33833415-Lecture-2-locating-roots-of-equations.html">LECTURE 2: Locating roots of equations</a>
    </h3>
    <p>
        <img alt="LECTURE 2: Locating roots of equations" title="LECTURE 2: Locating roots of equations" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33833415.jpg">
        1 LECTURE 2: Locating roots of equations September 24, 2012 In this chapter, we consider the task of solving equations numerically. The problem can be written out in the following form: f (x) = 0 where    </p>
    <a href="http://docplayer.net/33833415-Lecture-2-locating-roots-of-equations.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33751828-Tree-based-discretization-for-continuous-state-space-reinforcement-learning.html">Tree Based Discretization for Continuous State Space Reinforcement Learning</a>
    </h3>
    <p>
        <img alt="Tree Based Discretization for Continuous State Space Reinforcement Learning" title="Tree Based Discretization for Continuous State Space Reinforcement Learning" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33751828.jpg">
        Tree Based Discretization for Continuous State Space Reinforcement Learning William T B Uther and Manuela M Veloso Computer Science Department Carnegie Mellon University Pittsburgh, PA 1513 uther,veloso    </p>
    <a href="http://docplayer.net/33751828-Tree-based-discretization-for-continuous-state-space-reinforcement-learning.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35214317-Between-mdps-and-semi-mdps-a-framework-for-temporal-abstraction-in-reinforcement-learning.html">Between MDPs and semi-mdps: A framework for temporal abstraction in reinforcement learning</a>
    </h3>
    <p>
        <img alt="Between MDPs and semi-mdps: A framework for temporal abstraction in reinforcement learning" title="Between MDPs and semi-mdps: A framework for temporal abstraction in reinforcement learning" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35214317.jpg">
        Artificial Intelligence 112 (1999) 181 211 Between MDPs and semi-mdps: A framework for temporal abstraction in reinforcement learning Richard S. Sutton a,, Doina Precup b, Satinder Singh a a AT&amp;T Labs.-Research,    </p>
    <a href="http://docplayer.net/35214317-Between-mdps-and-semi-mdps-a-framework-for-temporal-abstraction-in-reinforcement-learning.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/27024580-Section-notes-3-the-simplex-algorithm-applied-math-121-week-of-february-14-2011.html">Section Notes 3. The Simplex Algorithm. Applied Math 121. Week of February 14, 2011</a>
    </h3>
    <p>
        <img alt="Section Notes 3. The Simplex Algorithm. Applied Math 121. Week of February 14, 2011" title="Section Notes 3. The Simplex Algorithm. Applied Math 121. Week of February 14, 2011" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/27024580.jpg">
        Section Notes 3 The Simplex Algorithm Applied Math 121 Week of February 14, 2011 Goals for the week understand how to get from an LP to a simplex tableau. be familiar with reduced costs, optimal solutions,    </p>
    <a href="http://docplayer.net/27024580-Section-notes-3-the-simplex-algorithm-applied-math-121-week-of-february-14-2011.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/8254169-Td-0-leads-to-better-policies-than-approximate-value-iteration.html">TD(0) Leads to Better Policies than Approximate Value Iteration</a>
    </h3>
    <p>
        <img alt="TD(0) Leads to Better Policies than Approximate Value Iteration" title="TD(0) Leads to Better Policies than Approximate Value Iteration" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/8254169.jpg">
        TD(0) Leads to Better Policies than Approximate Value Iteration Benjamin Van Roy Management Science and Engineering and Electrical Engineering Stanford University Stanford, CA 94305 bvr@stanford.edu Abstract    </p>
    <a href="http://docplayer.net/8254169-Td-0-leads-to-better-policies-than-approximate-value-iteration.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34812097-The-k-shortest-paths-approach-to-approximate-dynamic-programming.html">The K-Shortest Paths Approach to Approximate Dynamic Programming</a>
    </h3>
    <p>
        <img alt="The K-Shortest Paths Approach to Approximate Dynamic Programming" title="The K-Shortest Paths Approach to Approximate Dynamic Programming" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34812097.jpg">
        The K-Shortest Paths Approach to Approximate Dynamic Programming with applications to financial decision-making Nicolas Chapados 1 Yoshua Bengio 1 1 Département d informatique et recherche opérationnelle    </p>
    <a href="http://docplayer.net/34812097-The-k-shortest-paths-approach-to-approximate-dynamic-programming.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/30103012-Mixture-models-jia-li-department-of-statistics-the-pennsylvania-state-university-mixture-models.html">Mixture Models. Jia Li. Department of Statistics The Pennsylvania State University. Mixture Models</a>
    </h3>
    <p>
        <img alt="Mixture Models. Jia Li. Department of Statistics The Pennsylvania State University. Mixture Models" title="Mixture Models. Jia Li. Department of Statistics The Pennsylvania State University. Mixture Models" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/30103012.jpg">
        Mixture Models Department of Statistics The Pennsylvania State University Email: jiali@stat.psu.edu Clustering by Mixture Models General bacground on clustering Example method: -means Mixture model based    </p>
    <a href="http://docplayer.net/30103012-Mixture-models-jia-li-department-of-statistics-the-pennsylvania-state-university-mixture-models.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35574936-Applying-reinforcement-learning-to-obstacle-avoidance.html">Applying Reinforcement Learning to Obstacle Avoidance</a>
    </h3>
    <p>
        <img alt="Applying Reinforcement Learning to Obstacle Avoidance" title="Applying Reinforcement Learning to Obstacle Avoidance" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35574936.jpg">
        Applying Reinforcement Learning to Obstacle Avoidance Josh Beitelspacher University of Oklahoma, 308 Cate Center Drive Box 5242, Norman, OK 73072 USA josh@cs.ou.edu Abstract This paper applies reinforcement    </p>
    <a href="http://docplayer.net/35574936-Applying-reinforcement-learning-to-obstacle-avoidance.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35133723-Simulation-based-design-improvement-of-a-superconductive-magnet-by-mixed-integer-nonlinear-surrogate-optimization.html">Simulation-based design improvement of a superconductive magnet by mixed-integer nonlinear surrogate optimization</a>
    </h3>
    <p>
        <img alt="Simulation-based design improvement of a superconductive magnet by mixed-integer nonlinear surrogate optimization" title="Simulation-based design improvement of a superconductive magnet by mixed-integer nonlinear surrogate optimization" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35133723.jpg">
        Simulation-based design improvement of a superconductive magnet by mixed-integer nonlinear surrogate optimization T. Hemker, O. von Stryk, H. De Gersem, and T. Weiland The numerical optimization of continuous    </p>
    <a href="http://docplayer.net/35133723-Simulation-based-design-improvement-of-a-superconductive-magnet-by-mixed-integer-nonlinear-surrogate-optimization.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/29939882-Gradient-subgradient-and-how-they-may-affect-your-grade-ient.html">Gradient, Subgradient and how they may affect your grade(ient)</a>
    </h3>
    <p>
        <img alt="Gradient, Subgradient and how they may affect your grade(ient)" title="Gradient, Subgradient and how they may affect your grade(ient)" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/29939882.jpg">
        Gradient, Subgradient and how they may affect your grade(ient) David Sontag &amp; Yoni Halpern February 7, 2016 1 Introduction The goal of this note is to give background on convex optimization and the Pegasos    </p>
    <a href="http://docplayer.net/29939882-Gradient-subgradient-and-how-they-may-affect-your-grade-ient.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/20494949-Predictive-q-routing-a-memory-based-reinforcement-learning-approach-to-adaptive-traffic-control.html">Predictive Q-Routing: A Memory-based Reinforcement Learning Approach to Adaptive Traffic Control</a>
    </h3>
    <p>
        <img alt="Predictive Q-Routing: A Memory-based Reinforcement Learning Approach to Adaptive Traffic Control" title="Predictive Q-Routing: A Memory-based Reinforcement Learning Approach to Adaptive Traffic Control" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/20494949.jpg">
        Predictive Q-Routing: A Memory-based Reinforcement Learning Approach to Adaptive Traffic Control Samuel P.M. Choi, Dit-Yan Yeung Department of Computer Science Hong Kong University of Science and Technology    </p>
    <a href="http://docplayer.net/20494949-Predictive-q-routing-a-memory-based-reinforcement-learning-approach-to-adaptive-traffic-control.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33161841-Value-function-iteration.html">Value Function Iteration</a>
    </h3>
    <p>
        <img alt="Value Function Iteration" title="Value Function Iteration" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33161841.jpg">
        LECTURE NOTES COMPUTATIONAL METHODS FOR MACROECONOMICS FALL 2006 KAREN A. KOPECKY Value Function Iteration 1 Value Function Iteration Consider the standard neoclassical growth model in recursive form,    </p>
    <a href="http://docplayer.net/33161841-Value-function-iteration.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/30303910-Transportation-problems.html">TRANSPORTATION PROBLEMS</a>
    </h3>
    <p>
        <img alt="TRANSPORTATION PROBLEMS" title="TRANSPORTATION PROBLEMS" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/30303910.jpg">
        Chapter 5 TRANSPORTATION PROBLEMS 5.1. Classic transportation problem 5.1.1. The problem The standard form of the transportation problem is as follows: inf m n c ij x ij, where: i=1 j=1 n x ij = a i, 1    </p>
    <a href="http://docplayer.net/30303910-Transportation-problems.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33831711-1-review-of-fixed-point-iterations.html">1 Review of Fixed Point Iterations</a>
    </h3>
    <p>
        <img alt="1 Review of Fixed Point Iterations" title="1 Review of Fixed Point Iterations" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33831711.jpg">
        cs412: introduction to numerical analysis 09/21/10 Lecture 4: Solving Equations: Newton s Method, Bisection, and the Secant Method Instructor: Professor Amos Ron Scribes: Yunpeng Li, Mark Cowlishaw, Nathanael    </p>
    <a href="http://docplayer.net/33831711-1-review-of-fixed-point-iterations.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34128243-Integrated-architectures-for-learning-planning-and-reacting-based-on-approximating-dynamic-programming.html">Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming</a>
    </h3>
    <p>
        <img alt="Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming" title="Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34128243.jpg">
        Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming Richard S. Sutton Presented by Pirooz Chubak CMPUT 651 Course October 17 th Outline Dyna Architecture    </p>
    <a href="http://docplayer.net/34128243-Integrated-architectures-for-learning-planning-and-reacting-based-on-approximating-dynamic-programming.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34930206-Subthreshold-dynamics-of-hodgkin-huxley-and-izhikevich-spiking-neuron-models.html">Subthreshold dynamics of Hodgkin-Huxley and Izhikevich spiking neuron models</a>
    </h3>
    <p>
        <img alt="Subthreshold dynamics of Hodgkin-Huxley and Izhikevich spiking neuron models" title="Subthreshold dynamics of Hodgkin-Huxley and Izhikevich spiking neuron models" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34930206.jpg">
        Subthreshold dynamics of Hodgkin-Huxley and Izhikevich spiking neuron models Matthew D. Boardman Faculty of Computer Science Dalhousie University Matt.Boardman@dal.ca Abstract We compare the subthreshold    </p>
    <a href="http://docplayer.net/34930206-Subthreshold-dynamics-of-hodgkin-huxley-and-izhikevich-spiking-neuron-models.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34557256-Lecture-more-common-geometric-view-of-the-simplex-method.html">Lecture More common geometric view of the simplex method</a>
    </h3>
    <p>
        <img alt="Lecture More common geometric view of the simplex method" title="Lecture More common geometric view of the simplex method" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34557256.jpg">
        18.409 The Behavior of Algorithms in Practice 9 Apr. 2002 Lecture 14 Lecturer: Dan Spielman Scribe: Brian Sutton In this lecture, we study the worst-case complexity of the simplex method. We conclude by    </p>
    <a href="http://docplayer.net/34557256-Lecture-more-common-geometric-view-of-the-simplex-method.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/21687174-Reinforcement-learning.html">Reinforcement Learning</a>
    </h3>
    <p>
        <img alt="Reinforcement Learning" title="Reinforcement Learning" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/21687174.jpg">
        Reinforcement Learning LU 2 - Markov Decision Problems and Dynamic Programming Dr. Joschka Bödecker AG Maschinelles Lernen und Natürlichsprachliche Systeme Albert-Ludwigs-Universität Freiburg jboedeck@informatik.uni-freiburg.de    </p>
    <a href="http://docplayer.net/21687174-Reinforcement-learning.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35694673-Gas-portfolio-and-transport-optimization.html">Gas Portfolio and Transport Optimization</a>
    </h3>
    <p>
        <img alt="Gas Portfolio and Transport Optimization" title="Gas Portfolio and Transport Optimization" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35694673.jpg">
        Gas Portfolio and Transport Optimization Gido A.J.F. Brouns Alexander F. Boogert March 1, 2006 / 3467 words, 4 figures 1 Introduction The transport of natural gas has received significant attention in    </p>
    <a href="http://docplayer.net/35694673-Gas-portfolio-and-transport-optimization.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/31376376-Duality-uri-feige-november-17-2011.html">Duality. Uri Feige. November 17, 2011</a>
    </h3>
    <p>
        <img alt="Duality. Uri Feige. November 17, 2011" title="Duality. Uri Feige. November 17, 2011" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/31376376.jpg">
        Duality Uri Feige November 17, 2011 1 Linear programming duality 1.1 The diet problem revisited Recall the diet problem from Lecture 1. There are n foods, m nutrients, and a person (the buyer) is required    </p>
    <a href="http://docplayer.net/31376376-Duality-uri-feige-november-17-2011.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35194274-Dynamic-programming-c-h-a-p-t-e-r-9.html">Dynamic Programming C H A P T E R 9</a>
    </h3>
    <p>
        <img alt="Dynamic Programming C H A P T E R 9" title="Dynamic Programming C H A P T E R 9" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35194274.jpg">
        C H A P T E R 9 Dynamic Programming In chapter 2, we spent some time thinking about the phase portrait of the simple pendulum, and concluded with a challenge: can we design a nonlinear controller to reshape    </p>
    <a href="http://docplayer.net/35194274-Dynamic-programming-c-h-a-p-t-e-r-9.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34200885-Internal-rewards-mitigate-agent-boundedness.html">Internal Rewards Mitigate Agent Boundedness</a>
    </h3>
    <p>
        <img alt="Internal Rewards Mitigate Agent Boundedness" title="Internal Rewards Mitigate Agent Boundedness" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34200885.jpg">
        Jonathan Sorg jdsorg@umich.edu Satinder Singh baveja@umich.edu Computer Science &amp; Engineering, University of Michigan, 2260 Hayward Street, Ann Arbor, MI 48109 Richard Lewis rickl@umich.edu Department    </p>
    <a href="http://docplayer.net/34200885-Internal-rewards-mitigate-agent-boundedness.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/30181764-Jeep-problem-and-its-2d-extension-the-solution-with-an-application-of-the-reinforcement-learning.html">Jeep problem and its 2D extension the solution with an application of the reinforcement learning</a>
    </h3>
    <p>
        <img alt="Jeep problem and its 2D extension the solution with an application of the reinforcement learning" title="Jeep problem and its 2D extension the solution with an application of the reinforcement learning" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/30181764.jpg">
        Jeep problem and its 2D extension the solution with an application of the reinforcement learning Marcin Pluciński Faculty of Computer Science and Information Technology, Szczecin University of Technology,    </p>
    <a href="http://docplayer.net/30181764-Jeep-problem-and-its-2d-extension-the-solution-with-an-application-of-the-reinforcement-learning.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/29934769-Optimal-online-preemptive-scheduling.html">Optimal Online Preemptive Scheduling</a>
    </h3>
    <p>
        <img alt="Optimal Online Preemptive Scheduling" title="Optimal Online Preemptive Scheduling" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/29934769.jpg">
        IEOR 8100: Scheduling Lecture Guest Optimal Online Preemptive Scheduling Lecturer: Jir Sgall Scribe: Michael Hamilton 1 Introduction In this lecture we ll study online preemptive scheduling on m machines    </p>
    <a href="http://docplayer.net/29934769-Optimal-online-preemptive-scheduling.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35356626-Linear-algebra-linear-systems-of-differential-equations.html">LINEAR ALGEBRA LINEAR SYSTEMS OF DIFFERENTIAL EQUATIONS</a>
    </h3>
    <p>
        <img alt="LINEAR ALGEBRA LINEAR SYSTEMS OF DIFFERENTIAL EQUATIONS" title="LINEAR ALGEBRA LINEAR SYSTEMS OF DIFFERENTIAL EQUATIONS" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35356626.jpg">
        64:244:7 9 SPRING 2 Notes on LINEAR ALGEBRA with a few remarks on LINEAR SYSTEMS OF DIFFERENTIAL EQUATIONS Systems of linear equations Suppose we are given a system of m linear equations in n unknowns    </p>
    <a href="http://docplayer.net/35356626-Linear-algebra-linear-systems-of-differential-equations.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/21005466-Neural-fitted-q-iteration-first-experiences-with-a-data-efficient-neural-reinforcement-learning-method.html">Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method</a>
    </h3>
    <p>
        <img alt="Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method" title="Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/21005466.jpg">
        Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method Martin Riedmiller Neuroinformatics Group, University of Onsabrück, 49078 Osnabrück Abstract. This    </p>
    <a href="http://docplayer.net/21005466-Neural-fitted-q-iteration-first-experiences-with-a-data-efficient-neural-reinforcement-learning-method.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/25856867-Lecture-9-1-introduction-2-random-walks-in-graphs-1-1-how-to-explore-a-graph-cs-621-theory-gems-october-17-2012.html">Lecture 9. 1 Introduction. 2 Random Walks in Graphs. 1.1 How To Explore a Graph? CS-621 Theory Gems October 17, 2012</a>
    </h3>
    <p>
        <img alt="Lecture 9. 1 Introduction. 2 Random Walks in Graphs. 1.1 How To Explore a Graph? CS-621 Theory Gems October 17, 2012" title="Lecture 9. 1 Introduction. 2 Random Walks in Graphs. 1.1 How To Explore a Graph? CS-621 Theory Gems October 17, 2012" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/25856867.jpg">
        CS-62 Theory Gems October 7, 202 Lecture 9 Lecturer: Aleksander Mądry Scribes: Dorina Thanou, Xiaowen Dong Introduction Over the next couple of lectures, our focus will be on graphs. Graphs are one of    </p>
    <a href="http://docplayer.net/25856867-Lecture-9-1-introduction-2-random-walks-in-graphs-1-1-how-to-explore-a-graph-cs-621-theory-gems-october-17-2012.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/18039610-Using-markov-decision-processes-to-solve-a-portfolio-allocation-problem.html">Using Markov Decision Processes to Solve a Portfolio Allocation Problem</a>
    </h3>
    <p>
        <img alt="Using Markov Decision Processes to Solve a Portfolio Allocation Problem" title="Using Markov Decision Processes to Solve a Portfolio Allocation Problem" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/18039610.jpg">
        Using Markov Decision Processes to Solve a Portfolio Allocation Problem Daniel Bookstaber April 26, 2005 Contents 1 Introduction 3 2 Defining the Model 4 2.1 The Stochastic Model for a Single Asset.........................    </p>
    <a href="http://docplayer.net/18039610-Using-markov-decision-processes-to-solve-a-portfolio-allocation-problem.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/32639797-Lecture-8-september-25.html">Lecture 8 September 25</a>
    </h3>
    <p>
        <img alt="Lecture 8 September 25" title="Lecture 8 September 25" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/32639797.jpg">
        EE 381V: Large Scale Optimization Fall 2012 Lecture 8 September 25 Lecturer: Caramanis &amp; Sanghavi Scribe: Srinadh B, Anish Mittal 8.1 Newton s method 8.1.1 Convergence In the last class we have seen the    </p>
    <a href="http://docplayer.net/32639797-Lecture-8-september-25.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/24083927-Notes-improvements-on-learning-tetris-with-cross-entropy-abstract.html">NOTES IMPROVEMENTS ON LEARNING TETRIS WITH CROSS-ENTROPY ABSTRACT</a>
    </h3>
    <p>
        <img alt="NOTES IMPROVEMENTS ON LEARNING TETRIS WITH CROSS-ENTROPY ABSTRACT" title="NOTES IMPROVEMENTS ON LEARNING TETRIS WITH CROSS-ENTROPY ABSTRACT" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/24083927.jpg">
        Author manuscript, published in "N/P" Improvements on learning Tetris with the Cross-Entropy method 23 NOTES IMPROVEMENTS ON LEARNING TETRIS WITH CROSS-ENTROPY Christophe Thiery 1 and Bruno Scherrer 2    </p>
    <a href="http://docplayer.net/24083927-Notes-improvements-on-learning-tetris-with-cross-entropy-abstract.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/26537359-A-semi-parametric-approach-for-decomposition-of-absorption-spectra-in-the-presence-of-unknown-components.html">A Semi-parametric Approach for Decomposition of Absorption Spectra in the Presence of Unknown Components</a>
    </h3>
    <p>
        <img alt="A Semi-parametric Approach for Decomposition of Absorption Spectra in the Presence of Unknown Components" title="A Semi-parametric Approach for Decomposition of Absorption Spectra in the Presence of Unknown Components" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/26537359.jpg">
        A Semi-parametric Approach for Decomposition of Absorption Spectra in the Presence of Unknown Components Payman Sadegh 1,2, Henrik Aalborg Nielsen 1, and Henrik Madsen 1 Abstract Decomposition of absorption    </p>
    <a href="http://docplayer.net/26537359-A-semi-parametric-approach-for-decomposition-of-absorption-spectra-in-the-presence-of-unknown-components.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/32600079-Introduction-to-numerical-integration.html">Introduction to Numerical Integration</a>
    </h3>
    <p>
        <img alt="Introduction to Numerical Integration" title="Introduction to Numerical Integration" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/32600079.jpg">
        1 Introduction Introduction to Numerical Integration James R. Nagel Department of Electrical and Computer Engineering University of Utah, Salt Lake City, Utah February 4, 2012 By definition, the integral    </p>
    <a href="http://docplayer.net/32600079-Introduction-to-numerical-integration.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35094153-Restoring-focus-in-photographs-extraction-of-the-point-spread-function.html">Restoring focus in photographs: extraction of the point-spread function</a>
    </h3>
    <p>
        <img alt="Restoring focus in photographs: extraction of the point-spread function" title="Restoring focus in photographs: extraction of the point-spread function" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35094153.jpg">
        Restoring focus in photographs: extraction of the point-spread function Jeff Keacher December 12, 2008 1 Introduction Out-of-focus images are the bane of the modern photographer. While error in exposure,    </p>
    <a href="http://docplayer.net/35094153-Restoring-focus-in-photographs-extraction-of-the-point-spread-function.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/21254298-Creating-a-nl-texas-hold-em-bot.html">Creating a NL Texas Hold em Bot</a>
    </h3>
    <p>
        <img alt="Creating a NL Texas Hold em Bot" title="Creating a NL Texas Hold em Bot" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/21254298.jpg">
        Creating a NL Texas Hold em Bot Introduction Poker is an easy game to learn by very tough to master. One of the things that is hard to do is controlling emotions. Due to frustration, many have made the    </p>
    <a href="http://docplayer.net/21254298-Creating-a-nl-texas-hold-em-bot.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/32549901-Machine-learning-michaelmas-term-2016-lecture-8-classification-logistic-regression.html">Machine Learning - Michaelmas Term 2016 Lecture 8 : Classification: Logistic Regression</a>
    </h3>
    <p>
        <img alt="Machine Learning - Michaelmas Term 2016 Lecture 8 : Classification: Logistic Regression" title="Machine Learning - Michaelmas Term 2016 Lecture 8 : Classification: Logistic Regression" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/32549901.jpg">
        Machine Learning - Michaelmas Term 2016 Lecture 8 : Classification: Logistic Regression Lecturer: Varun Kanade In the previous lecture, we studied two different generative models for classification Naïve    </p>
    <a href="http://docplayer.net/32549901-Machine-learning-michaelmas-term-2016-lecture-8-classification-logistic-regression.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/21565514-Temporal-difference-learning-in-the-tetris-game.html">Temporal Difference Learning in the Tetris Game</a>
    </h3>
    <p>
        <img alt="Temporal Difference Learning in the Tetris Game" title="Temporal Difference Learning in the Tetris Game" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/21565514.jpg">
        Temporal Difference Learning in the Tetris Game Hans Pirnay, Slava Arabagi February 6, 2009 1 Introduction Learning to play the game Tetris has been a common challenge on a few past machine learning competitions.    </p>
    <a href="http://docplayer.net/21565514-Temporal-difference-learning-in-the-tetris-game.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/32090065-Cs-229-autumn-2016-problem-set-3-theory-unsupervised-learning.html">CS 229 Autumn 2016 Problem Set#3:Theory &amp; Unsupervised learning</a>
    </h3>
    <p>
        <img alt="CS 229 Autumn 2016 Problem Set#3:Theory &amp; Unsupervised learning" title="CS 229 Autumn 2016 Problem Set#3:Theory &amp; Unsupervised learning" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/32090065.jpg">
        CS229 Problem Set #3 1 CS 229 Autumn 2016 Problem Set#3:Theory &amp; Unsupervised learning Due Wednesday, November 16 at 11:00 am on Gradescope. Notes: (1) These questions require thought, but do not require    </p>
    <a href="http://docplayer.net/32090065-Cs-229-autumn-2016-problem-set-3-theory-unsupervised-learning.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35009069-Learning-shaping-rewards-in-model-based-reinforcement-learning.html">Learning Shaping Rewards in Model-based Reinforcement. learning</a>
    </h3>
    <p>
        <img alt="Learning Shaping Rewards in Model-based Reinforcement. learning" title="Learning Shaping Rewards in Model-based Reinforcement. learning" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35009069.jpg">
        Learning Shaping Rewards in Model-based Reinforcement Learning Marek Grzes and Daniel Kudenko Department of Computer Science University of York York, YO10 5DD, UK {grzes, kudenko}@cs.york.ac.uk ABSTRACT    </p>
    <a href="http://docplayer.net/35009069-Learning-shaping-rewards-in-model-based-reinforcement-learning.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34538358-Chapter-8-hw-solution.html">Chapter 8 HW Solution</a>
    </h3>
    <p>
        <img alt="Chapter 8 HW Solution" title="Chapter 8 HW Solution" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34538358.jpg">
        Review Questions. Chapter 8 HW Solution 1. What is a root locus? A plot of the possible closed-loop pole locations as some parameter varies from 0 to. 4. Do the zeros of a system change with a change in    </p>
    <a href="http://docplayer.net/34538358-Chapter-8-hw-solution.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/11950428-How-i-won-the-chess-ratings-elo-vs-the-rest-of-the-world-competition.html">How I won the Chess Ratings: Elo vs the rest of the world Competition</a>
    </h3>
    <p>
        <img alt="How I won the Chess Ratings: Elo vs the rest of the world Competition" title="How I won the Chess Ratings: Elo vs the rest of the world Competition" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/11950428.jpg">
        How I won the Chess Ratings: Elo vs the rest of the world Competition Yannis Sismanis November 2010 Abstract This article discusses in detail the rating system that won the kaggle competition Chess Ratings:    </p>
    <a href="http://docplayer.net/11950428-How-i-won-the-chess-ratings-elo-vs-the-rest-of-the-world-competition.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/29277817-Tree-based-batch-mode-reinforcement-learning.html">Tree-Based Batch Mode Reinforcement Learning</a>
    </h3>
    <p>
        <img alt="Tree-Based Batch Mode Reinforcement Learning" title="Tree-Based Batch Mode Reinforcement Learning" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/29277817.jpg">
        Journal of Machine Learning Research 6 (2005) 503 556 Submitted 11/03; Revised 4/04; Published 4/05 Tree-Based Batch Mode Reinforcement Learning Damien Ernst Pierre Geurts Louis Wehenkel Department of    </p>
    <a href="http://docplayer.net/29277817-Tree-based-batch-mode-reinforcement-learning.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/13193391-Chapter-4-artificial-neural-networks.html">Chapter 4: Artificial Neural Networks</a>
    </h3>
    <p>
        <img alt="Chapter 4: Artificial Neural Networks" title="Chapter 4: Artificial Neural Networks" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/13193391.jpg">
        Chapter 4: Artificial Neural Networks CS 536: Machine Learning Littman (Wu, TA) Administration icml-03: instructional Conference on Machine Learning http://www.cs.rutgers.edu/~mlittman/courses/ml03/icml03/    </p>
    <a href="http://docplayer.net/13193391-Chapter-4-artificial-neural-networks.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34113622-Data-fitting-and-linear-least-squares-problems.html">Data Fitting and Linear Least-Squares Problems</a>
    </h3>
    <p>
        <img alt="Data Fitting and Linear Least-Squares Problems" title="Data Fitting and Linear Least-Squares Problems" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34113622.jpg">
        02610 Optimization and Data Fitting Linear Data Fitting Problems 1 Data Fitting and Linear Least-Squares Problems This lecture is based on the book P. C. Hansen, V. Pereyra and G. Scherer, Least Squares    </p>
    <a href="http://docplayer.net/34113622-Data-fitting-and-linear-least-squares-problems.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/24853904-Optimization-and-operations-research-vol-iv-markov-decision-processes-ulrich-rieder.html">OPTIMIZATION AND OPERATIONS RESEARCH Vol. IV - Markov Decision Processes - Ulrich Rieder</a>
    </h3>
    <p>
        <img alt="OPTIMIZATION AND OPERATIONS RESEARCH Vol. IV - Markov Decision Processes - Ulrich Rieder" title="OPTIMIZATION AND OPERATIONS RESEARCH Vol. IV - Markov Decision Processes - Ulrich Rieder" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/24853904.jpg">
        MARKOV DECISIO PROCESSES Ulrich Rieder University of Ulm, Germany Keywords: Markov decision problem, stochastic dynamic program, total reward criteria, average reward, optimal policy, optimality equation,    </p>
    <a href="http://docplayer.net/24853904-Optimization-and-operations-research-vol-iv-markov-decision-processes-ulrich-rieder.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35187499-Computational-economics-session-17-numerical-dynamic-programming.html">Computational Economics. Session 17: Numerical Dynamic Programming</a>
    </h3>
    <p>
        <img alt="Computational Economics. Session 17: Numerical Dynamic Programming" title="Computational Economics. Session 17: Numerical Dynamic Programming" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35187499.jpg">
        Computational Economics Session 17: Numerical Dynamic Programming Agenda Discrete-time dynamic programming. Continuous-time dynamic programming. Methods for finite-state problems: Value function iteration.    </p>
    <a href="http://docplayer.net/35187499-Computational-economics-session-17-numerical-dynamic-programming.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/17652440-Learning-exercise-policies-for-american-options.html">Learning Exercise Policies for American Options</a>
    </h3>
    <p>
        <img alt="Learning Exercise Policies for American Options" title="Learning Exercise Policies for American Options" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/17652440.jpg">
        Yuxi Li Dept. of Computing Science University of Alberta Edmonton, Alberta Canada T6G 2E8 Csaba Szepesvari Dept. of Computing Science University of Alberta Edmonton, Alberta Canada T6G 2E8 Dale Schuurmans    </p>
    <a href="http://docplayer.net/17652440-Learning-exercise-policies-for-american-options.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34243425-Stochastic-structural-dynamics-prof-dr-c-s-manohar-department-of-civil-engineering-indian-institute-of-science-bangalore.html">Stochastic Structural Dynamics Prof. Dr. C. S. Manohar Department of Civil Engineering Indian Institute of Science, Bangalore</a>
    </h3>
    <p>
        <img alt="Stochastic Structural Dynamics Prof. Dr. C. S. Manohar Department of Civil Engineering Indian Institute of Science, Bangalore" title="Stochastic Structural Dynamics Prof. Dr. C. S. Manohar Department of Civil Engineering Indian Institute of Science, Bangalore" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34243425.jpg">
        Stochastic Structural Dynamics Prof. Dr. C. S. Manohar Department of Civil Engineering Indian Institute of Science, Bangalore (Refer Slide Time: 00:23) Lecture No. # 3 Scalar random variables-2 Will begin    </p>
    <a href="http://docplayer.net/34243425-Stochastic-structural-dynamics-prof-dr-c-s-manohar-department-of-civil-engineering-indian-institute-of-science-bangalore.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34315486-Advanced-edge-detection-techniques.html">Advanced Edge Detection Techniques</a>
    </h3>
    <p>
        <img alt="Advanced Edge Detection Techniques" title="Advanced Edge Detection Techniques" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34315486.jpg">
        Today s Topics Written assignment due today Today: More on edge detection Edge detection using zero-crossings Scale-space techniques Case study: Intelligent Scissors Advanced Edge Detection Techniques    </p>
    <a href="http://docplayer.net/34315486-Advanced-edge-detection-techniques.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33978325-Experiments-with-improved-approximate-mean-value-analysis-algorithms.html">Experiments with Improved Approximate Mean Value Analysis Algorithms</a>
    </h3>
    <p>
        <img alt="Experiments with Improved Approximate Mean Value Analysis Algorithms" title="Experiments with Improved Approximate Mean Value Analysis Algorithms" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33978325.jpg">
        Experiments with Improved Approximate Mean Value Analysis Algorithms Hai Wang and Kenneth C. Sevcik Department of Computer Science University of Toronto Toronto, Ontario, Canada M5S 3G4 {hai,kcs}@cs.toronto.edu    </p>
    <a href="http://docplayer.net/33978325-Experiments-with-improved-approximate-mean-value-analysis-algorithms.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/17229441-Lecture-5-model-free-control.html">Lecture 5: Model-Free Control</a>
    </h3>
    <p>
        <img alt="Lecture 5: Model-Free Control" title="Lecture 5: Model-Free Control" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/17229441.jpg">
        Lecture 5: Model-Free Control David Silver Outline 1 Introduction 2 On-Policy Monte-Carlo Control 3 On-Policy Temporal-Difference Learning 4 Off-Policy Learning 5 Summary Introduction Model-Free Reinforcement    </p>
    <a href="http://docplayer.net/17229441-Lecture-5-model-free-control.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/32591070-Variational-inference.html">Variational Inference</a>
    </h3>
    <p>
        <img alt="Variational Inference" title="Variational Inference" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/32591070.jpg">
        Variational Inference David M. Blei 1 Set up As usual, we will assume that x = x 1:n are observations and z = z 1:m are hidden variables. We assume additional parameters α that are fixed. Note we are general    </p>
    <a href="http://docplayer.net/32591070-Variational-inference.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33970919-Chapter-3-tennis-style-detection.html">Chapter 3 Tennis Style Detection</a>
    </h3>
    <p>
        <img alt="Chapter 3 Tennis Style Detection" title="Chapter 3 Tennis Style Detection" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33970919.jpg">
        Chapter Tennis Style Detection.1 Experimental Design In this experiment, we designed a simple simulator of tennis, to study different people s playing styles. The ball is served automatically from a random    </p>
    <a href="http://docplayer.net/33970919-Chapter-3-tennis-style-detection.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/31258508-A-new-reinforcement-learning-algorithm.html">A NEW REINFORCEMENT LEARNING ALGORITHM</a>
    </h3>
    <p>
        <img alt="A NEW REINFORCEMENT LEARNING ALGORITHM" title="A NEW REINFORCEMENT LEARNING ALGORITHM" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/31258508.jpg">
        STUDIA UNIV. BABEŞ BOLYAI, INFORMATICA, Volume XLVIII, Number 1, 2003 A NEW REINFORCEMENT LEARNING ALGORITHM GABRIELA ŞERBAN Abstract. The field of Reinforcement Learning, a sub-field of machine learning,    </p>
    <a href="http://docplayer.net/31258508-A-new-reinforcement-learning-algorithm.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34721239-The-simple-algorithm-for-pressure-velocity-coupling.html">The SIMPLE Algorithm for Pressure-Velocity Coupling</a>
    </h3>
    <p>
        <img alt="The SIMPLE Algorithm for Pressure-Velocity Coupling" title="The SIMPLE Algorithm for Pressure-Velocity Coupling" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34721239.jpg">
        The SIMPLE Algorithm for Pressure-Velocity Coupling ME 448/548 Notes Gerald Recktenwald Portland State University Department of Mechanical Engineering gerry@pdx.edu February 10, 2014 ME 448/548 SIMPLE    </p>
    <a href="http://docplayer.net/34721239-The-simple-algorithm-for-pressure-velocity-coupling.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33831719-6-iterative-methods-for-linear-systems-the-stepwise-approach-to-the-solution-of-non-linear-equations.html">6. Iterative Methods for Linear Systems. The stepwise approach to the solution... of non-linear equations</a>
    </h3>
    <p>
        <img alt="6. Iterative Methods for Linear Systems. The stepwise approach to the solution... of non-linear equations" title="6. Iterative Methods for Linear Systems. The stepwise approach to the solution... of non-linear equations" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33831719.jpg">
        6. Iterative Methods for Linear Systems The stepwise approach to the solution... of non-linear equations The stepwise approach to the solution... of non-linear equations, January 17, 2013 1 6.4. Non-Linear    </p>
    <a href="http://docplayer.net/33831719-6-iterative-methods-for-linear-systems-the-stepwise-approach-to-the-solution-of-non-linear-equations.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/14088356-Introduction-to-logistic-regression.html">Introduction to Logistic Regression</a>
    </h3>
    <p>
        <img alt="Introduction to Logistic Regression" title="Introduction to Logistic Regression" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/14088356.jpg">
        OpenStax-CNX module: m42090 1 Introduction to Logistic Regression Dan Calderon This work is produced by OpenStax-CNX and licensed under the Creative Commons Attribution License 3.0 Abstract Gives introduction    </p>
    <a href="http://docplayer.net/14088356-Introduction-to-logistic-regression.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/29307375-November-28-th-carlos-guestrin-lower-dimensional-projections.html">November 28 th, Carlos Guestrin. Lower dimensional projections</a>
    </h3>
    <p>
        <img alt="November 28 th, Carlos Guestrin. Lower dimensional projections" title="November 28 th, Carlos Guestrin. Lower dimensional projections" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/29307375.jpg">
        PCA Machine Learning 070/578 Carlos Guestrin Carnegie Mellon University November 28 th, 2007 Lower dimensional projections Rather than picking a subset of the features, we can new features that are combinations    </p>
    <a href="http://docplayer.net/29307375-November-28-th-carlos-guestrin-lower-dimensional-projections.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/33504692-Chapter-9-newton-s-method.html">Chapter 9 Newton s Method</a>
    </h3>
    <p>
        <img alt="Chapter 9 Newton s Method" title="Chapter 9 Newton s Method" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/33504692.jpg">
        Chapter 9 Newton s Method An Introduction to Optimization Spring, 2014 Wei-Ta Chu 1 Introduction The steepest descent method uses only first derivatives in selecting a suitable search direction. Newton    </p>
    <a href="http://docplayer.net/33504692-Chapter-9-newton-s-method.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35227545-Hierarchical-model-based-reinforcement-learning-r-max-maxq.html">Hierarchical Model-Based Reinforcement Learning: R-MAX + MAXQ</a>
    </h3>
    <p>
        <img alt="Hierarchical Model-Based Reinforcement Learning: R-MAX + MAXQ" title="Hierarchical Model-Based Reinforcement Learning: R-MAX + MAXQ" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35227545.jpg">
        To appear in The 25th International Conference on Machine Learning (ICML 08), Helsinki, Finland, July 2008. Hierarchical Model-Based Reinforcement Learning: R-MAX + MAXQ Nicholas K. Jong The University    </p>
    <a href="http://docplayer.net/35227545-Hierarchical-model-based-reinforcement-learning-r-max-maxq.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/35167580-Finding-the-shortest-path-between-two-points-in-a-simple-polygon-by-applying-a-rubberband-algorithm.html">Finding the Shortest Path Between Two Points in a Simple Polygon By Applying a Rubberband Algorithm</a>
    </h3>
    <p>
        <img alt="Finding the Shortest Path Between Two Points in a Simple Polygon By Applying a Rubberband Algorithm" title="Finding the Shortest Path Between Two Points in a Simple Polygon By Applying a Rubberband Algorithm" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/35167580.jpg">
        Finding the Shortest Path Between Two Points in a Simple Polygon By Applying a Rubberband Algorithm Fajie Li and Reinhard Klette Computer Science Department, The University of Auckland Auckland, New Zealand    </p>
    <a href="http://docplayer.net/35167580-Finding-the-shortest-path-between-two-points-in-a-simple-polygon-by-applying-a-rubberband-algorithm.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/28071248-Solving-systems-of-linear-equations-over-finite-fields.html">Solving systems of linear equations over finite fields</a>
    </h3>
    <p>
        <img alt="Solving systems of linear equations over finite fields" title="Solving systems of linear equations over finite fields" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/28071248.jpg">
        Solving systems of linear equations over finite fields June 1, 2007 1 Introduction 2 3 4 Basic problem Introduction x 1 + x 2 + x 3 = 1 x 1 + x 2 = 1 x 1 + x 2 + x 4 = 1 x 2 + x 4 = 0 x 1 + x 3 + x 4 =    </p>
    <a href="http://docplayer.net/28071248-Solving-systems-of-linear-equations-over-finite-fields.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/32912413-Discrete-iterative-curve-alignment.html">Discrete Iterative Curve Alignment</a>
    </h3>
    <p>
        <img alt="Discrete Iterative Curve Alignment" title="Discrete Iterative Curve Alignment" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/32912413.jpg">
        Discrete Iterative Curve Alignment A Tool for Optimal Time Domain Calibration By Dr. Tim Butters Data Assimilation &amp; Numerical Analysis Specialist tim.butters@sabisu.co www.sabisu.co Contents 1 Introduction    </p>
    <a href="http://docplayer.net/32912413-Discrete-iterative-curve-alignment.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>
<div class="news-blocks">

    <h3>
        <a href="http://docplayer.net/34821048-Selective-block-minimization-for-faster-convergence-of-limited-memory-large-scale-linear-models.html">Selective Block Minimization for Faster Convergence of Limited Memory Large-Scale Linear Models</a>
    </h3>
    <p>
        <img alt="Selective Block Minimization for Faster Convergence of Limited Memory Large-Scale Linear Models" title="Selective Block Minimization for Faster Convergence of Limited Memory Large-Scale Linear Models" class="news-block-img pull-right" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/34821048.jpg">
        Selective Block Minimization for Faster Convergence of Limited Memory Large-Scale Linear Models Kai-Wei Chang Department of Computer Science University of Illinois at Urbana-Champaign Joint work with Dan    </p>
    <a href="http://docplayer.net/34821048-Selective-block-minimization-for-faster-convergence-of-limited-memory-large-scale-linear-models.html" class="news-block-btn">
        More information <i class="m-icon-swapright m-icon-black"></i>
    </a>
</div>

                            </div>
                            <!-- END RIGHT SIDEBAR -->


                        </div>
                    </div>
                </div>
                <!-- END CONTENT -->
            </div>
            <!-- END SIDEBAR & CONTENT -->
        </div>
    </div>
<!-- BEGIN FOOTER -->
<div class="footer">
    <div class="container">
        <div class="row">
            <!-- BEGIN COPYRIGHT -->
            <div class="col-md-10 col-sm-10 padding-top-10">
                2017 © DocPlayer.net <a href="http://docplayer.net/support/privacy-policy/">Privacy Policy</a> | <a href="http://docplayer.net/support/terms-of-service/">Terms of Service</a> | <a href="http://docplayer.net/support/feedback/">Feedback</a>
            </div>
            <!-- END COPYRIGHT -->

            <!-- BEGIN PAYMENTS -->
            <div class="col-md-6 col-sm-6">
                            </div>
            <!-- END PAYMENTS -->
        </div>
    </div>
</div>
<!-- END FOOTER -->

<!--[if lt IE 9]>
    <script src="/static/theme/global/plugins/respond.min.js"></script>
<![endif]-->


    <script src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/total.js" type="text/javascript"></script>

<div style="display: none;">
<script>
        var page_data = {"domain_id":38,"design_id":0,"page_id":0};        (function(){
            var img = new Image();
            var pixel_image_src = '/pix/'+Math.random()+'/report/pixel.gif?type=pageview&domain_id=38&page_id=0&design_id=0&l='+encodeURIComponent(navigator.language)+'&p='+encodeURIComponent(navigator.platform)+'&url='+encodeURIComponent(document.location);
            document.write('<img src="'+pixel_image_src+'"> ');
        }());
    </script><img src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/pixel.gif"> 
</div>


<script type="text/javascript">( function(){ window.SIG_EXT = {}; } )()</script><ins class="adsbygoogle" data-adsbygoogle-status="done" style="display: none; width: auto; height: auto;"><ins id="aswift_8_expand" style="display:inline-table;border:none;height:0px;margin:0;padding:0;position:relative;visibility:visible;width:0px;background-color:transparent"><ins id="aswift_8_anchor" style="display:block;border:none;height:0px;margin:0;padding:0;position:relative;visibility:visible;width:0px;background-color:transparent"><iframe width="0" height="0" frameborder="0" marginwidth="0" marginheight="0" vspace="0" hspace="0" allowtransparency="true" scrolling="no" allowfullscreen="true" onload="var i=this.id,s=window.google_iframe_oncopy,H=s&amp;&amp;s.handlers,h=H&amp;&amp;H[i],w=this.contentWindow,d;try{d=w.document}catch(e){}if(h&amp;&amp;d&amp;&amp;(!d.body||!d.body.firstChild)){if(h.call){setTimeout(h,0)}else if(h.match){try{h=s.upd(h,i)}catch(e){}w.location.replace(h)}}" id="aswift_8" name="aswift_8" style="left:0;position:absolute;top:0;" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/saved_resource(10).html"></iframe></ins></ins></ins><div id="topcontrol" title="Scroll Back to Top" style="position: fixed; bottom: 10px; right: 10px; cursor: pointer; opacity: 1;"><img src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/up.png" style="width:40px; height:40px"></div><div id="_atssh" style="visibility: hidden; height: 1px; width: 1px; position: absolute; top: -9999px; z-index: 100000;"><iframe id="_atssh765" title="AddThis utility frame" src="./Off-policy Learning with Linear Action Models_ An Efficient One-Collection-For-All Solution - PDF_files/sh.0d19417fd0a004d73df6a35b.html" style="height: 1px; width: 1px; position: absolute; top: 0px; z-index: 100000; border: 0px; left: 0px;"></iframe></div><style id="service-icons-0"></style><span style="border-radius: 3px; text-indent: 20px; width: auto; padding: 0px 4px 0px 0px; text-align: center; font-style: normal; font-variant: normal; font-weight: bold; font-stretch: normal; font-size: 11px; line-height: 20px; font-family: &quot;Helvetica Neue&quot;, Helvetica, sans-serif; color: rgb(255, 255, 255); background: url(&quot;data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMzBweCIgd2lkdGg9IjMwcHgiIHZpZXdCb3g9Ii0xIC0xIDMxIDMxIj48Zz48cGF0aCBkPSJNMjkuNDQ5LDE0LjY2MiBDMjkuNDQ5LDIyLjcyMiAyMi44NjgsMjkuMjU2IDE0Ljc1LDI5LjI1NiBDNi42MzIsMjkuMjU2IDAuMDUxLDIyLjcyMiAwLjA1MSwxNC42NjIgQzAuMDUxLDYuNjAxIDYuNjMyLDAuMDY3IDE0Ljc1LDAuMDY3IEMyMi44NjgsMC4wNjcgMjkuNDQ5LDYuNjAxIDI5LjQ0OSwxNC42NjIiIGZpbGw9IiNmZmYiIHN0cm9rZT0iI2ZmZiIgc3Ryb2tlLXdpZHRoPSIxIj48L3BhdGg+PHBhdGggZD0iTTE0LjczMywxLjY4NiBDNy41MTYsMS42ODYgMS42NjUsNy40OTUgMS42NjUsMTQuNjYyIEMxLjY2NSwyMC4xNTkgNS4xMDksMjQuODU0IDkuOTcsMjYuNzQ0IEM5Ljg1NiwyNS43MTggOS43NTMsMjQuMTQzIDEwLjAxNiwyMy4wMjIgQzEwLjI1MywyMi4wMSAxMS41NDgsMTYuNTcyIDExLjU0OCwxNi41NzIgQzExLjU0OCwxNi41NzIgMTEuMTU3LDE1Ljc5NSAxMS4xNTcsMTQuNjQ2IEMxMS4xNTcsMTIuODQyIDEyLjIxMSwxMS40OTUgMTMuNTIyLDExLjQ5NSBDMTQuNjM3LDExLjQ5NSAxNS4xNzUsMTIuMzI2IDE1LjE3NSwxMy4zMjMgQzE1LjE3NSwxNC40MzYgMTQuNDYyLDE2LjEgMTQuMDkzLDE3LjY0MyBDMTMuNzg1LDE4LjkzNSAxNC43NDUsMTkuOTg4IDE2LjAyOCwxOS45ODggQzE4LjM1MSwxOS45ODggMjAuMTM2LDE3LjU1NiAyMC4xMzYsMTQuMDQ2IEMyMC4xMzYsMTAuOTM5IDE3Ljg4OCw4Ljc2NyAxNC42NzgsOC43NjcgQzEwLjk1OSw4Ljc2NyA4Ljc3NywxMS41MzYgOC43NzcsMTQuMzk4IEM4Ljc3NywxNS41MTMgOS4yMSwxNi43MDkgOS43NDksMTcuMzU5IEM5Ljg1NiwxNy40ODggOS44NzIsMTcuNiA5Ljg0LDE3LjczMSBDOS43NDEsMTguMTQxIDkuNTIsMTkuMDIzIDkuNDc3LDE5LjIwMyBDOS40MiwxOS40NCA5LjI4OCwxOS40OTEgOS4wNCwxOS4zNzYgQzcuNDA4LDE4LjYyMiA2LjM4NywxNi4yNTIgNi4zODcsMTQuMzQ5IEM2LjM4NywxMC4yNTYgOS4zODMsNi40OTcgMTUuMDIyLDYuNDk3IEMxOS41NTUsNi40OTcgMjMuMDc4LDkuNzA1IDIzLjA3OCwxMy45OTEgQzIzLjA3OCwxOC40NjMgMjAuMjM5LDIyLjA2MiAxNi4yOTcsMjIuMDYyIEMxNC45NzMsMjIuMDYyIDEzLjcyOCwyMS4zNzkgMTMuMzAyLDIwLjU3MiBDMTMuMzAyLDIwLjU3MiAxMi42NDcsMjMuMDUgMTIuNDg4LDIzLjY1NyBDMTIuMTkzLDI0Ljc4NCAxMS4zOTYsMjYuMTk2IDEwLjg2MywyNy4wNTggQzEyLjA4NiwyNy40MzQgMTMuMzg2LDI3LjYzNyAxNC43MzMsMjcuNjM3IEMyMS45NSwyNy42MzcgMjcuODAxLDIxLjgyOCAyNy44MDEsMTQuNjYyIEMyNy44MDEsNy40OTUgMjEuOTUsMS42ODYgMTQuNzMzLDEuNjg2IiBmaWxsPSIjYmQwODFjIj48L3BhdGg+PC9nPjwvc3ZnPg==&quot;) 3px 50% / 14px 14px no-repeat rgb(189, 8, 28); position: absolute; opacity: 1; z-index: 8675309; display: none; cursor: pointer; border: none; -webkit-font-smoothing: antialiased;">Save</span><span style="border-radius: 3px; text-indent: 20px; width: 20px; height: 20px; background: url(&quot;data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/Pgo8c3ZnIHdpZHRoPSIzMHB4IiBoZWlnaHQ9IjMwcHgiIHZpZXdCb3g9IjAgMCAzMCAzMCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIj4KPGc+CjxwYXRoIHN0cm9rZT0iI2ZmZiIgc3Ryb2tlLXdpZHRoPSI0IiBkPSJNMTcsMTcgTDIyLDIyIFogIi8+CjxjaXJjbGUgc3Ryb2tlPSIjZmZmIiBjeD0iMTMiIGN5PSIxMyIgcj0iNiIgZmlsbD0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIzLjUiLz4KPHBhdGggZmlsbD0iI2ZmZiIgZD0iTTAsMCBMOCwwIEw4LDMgTDMsMyBMMyw4IEwwLDggWiIvPgo8cGF0aCBmaWxsPSIjZmZmIiBkPSJNMzAsMjIgTDMwLDMwIEwyMiwzMCBMMjIsMjcgTDI3LDI3IEwyNywyMiBaIi8+CjxwYXRoIGZpbGw9IiNmZmYiIGQ9Ik0zMCwwIEwzMCw4IEwyNyw4IEwyNywzIEwyMiwzIEwyMiwwIFoiLz4KPHBhdGggZmlsbD0iI2ZmZiIgZD0iTTAsMjIgTDMsMjIgTDMsMjcgTDgsMjcgTDgsMzAgTDAsMzBaIi8+CjwvZz4KPC9zdmc+Cg==&quot;) 50% 50% / 14px 14px no-repeat rgba(0, 0, 0, 0.4); position: absolute; opacity: 1; z-index: 8675309; display: none; cursor: pointer; border: none;"></span></body></html>