## Welcome!

My name is Hengshuai Yao. I am leading a team developing robots that reasons in Reinforcement Learning and Deep Learning at Huawei Canada. Before that, I was building an AI system for NCSoft in California. When doing my Ph.D at University of Alberta (2008--2014), I was working with Csaba Szepesvari, Rich Sutton, Dale Schuurmans, Davood Rafiei, Shalabh Bhatnagar, Xinhua Zhang and Chihoon Lee. I had spent two years working on a mobile app startup (6 team members) in Edmonton during and after my PhD studies.  

## Research

### Thesis
Yao, H. Model-based Reinforcement Learning with State and Action Abstractions. [Ph.D thesis](papers/yao_hengshuai_PhD.pdf), 2015. 

### Action-model based RL

- Yao, H. and Szepesvari, Cs. [Approximate Policy Iteration with Linear Action Models](papers/lamapi.pdf). Twenty-Sixth Conference on Artificial Intelligence. AAAI. Toronto, Canada. 2012. 
- Yao, H., Szepesvari, Cs., Pires, B. A., and Zhang, X. 2014. [Pseudo-MDPs and Factored Linear Action Models](papers/pmdp.pdf). IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (IEEE ADPRL), Best student paper nomination, Orlando, Florida, USA. 

### Dyna-style Planning
Multi-step linear Dyna style planning. 

- Yao, H., Sutton, R. S., Bhatnagar, S., Diao, D., and Szepesvari, Cs. [Dyna(k): A multi-step Dyna planning. Abstraction in Reinforcement Learning](papers/dynak.pdf). Montreal, Canada. June 2009. 
- Yao, H., Bhatnagar, S., and Diao, D. [Multi-step linear Dyna-style planning](papers/multi-step-dyna.pdf). Advances in Neural Information Processing Systems (NIPS) 22, Vancouver, BC, Canada. 2009.
- Yao, H. Linear least-squares Dyna-style planning. Technical Report TR11-04, Department of Computing Science, University of Alberta. 2011.

### Temporal Difference Learning
- Yao, H., and Liu, Z-Q. [Preconditioned temporal difference learning](papers/ptd.pdf). The 25th International Conference on Machine learning (ICML), Helsinki, Finland. June 2008. 
- Yao, H., and Liu, Z-Q. [Minimal residual approaches for policy evaluation in large sparse Markov chains](papers/mr.pdf). The Tenth International Symposium on Artificial Intelligence and Mathematics (ISAIM), Fort Lauderdale, USA. January 2008. 
- Yao, H., Bhatnagar, S., and Szepesvari, Cs. Temporal difference learning by direct preconditioning. Multidisciplinary Symposium on Reinforcement Learning (MSRL), Montreal, Canada. June 2009. 
- Yao, H., Bhatnagar, S., and Szepesvari, Cs. [LMS-2: towards an algorithm that is as cheap as LMS and almost as efficient as RLS](papers/lms2.pdf). The Forty-eighth IEEE Control and Decision Conference (CDC), Shanghai, China. December 2009.

### Off-policy Learning
- Yao, H. Off-policy learning with linear action models: an efficient "One-Collection-For-All-Solution". In workshop on "Planning and Acting with Uncertain Models" at the 28th ICML, Bellevue, Washington, USA. 2011. 

### Options
- Yao, H., Szepesvari, Cs., Sutton, R., and Bhatnagar,S. 2014. [Universal Option Models](papers/uom.pdf). NIPS. Montreal, Quebec, Canada. 

### Web Search
- Yao, H. and Schuurmans, D. 2013. [Reinforcement Ranking](papers/rr.pdf). 
- Lee, C., Yao, H., He, X., Su, C., and Chang, J-Y. 2014. [A System to Predict Future Popularity: Learning to Classify](papers/trending.pdf). WWW (poster), Seol,Korea. 
- Yao, H. 2012. [MaxRank: Discovering and Leveraging the Most Valuable Links for Ranking](papers/maxrank.pdf)
- Yao, H., Sutton R. and Rafiei D. [A Study of Temporal Citation Count Prediction using Reinforcement Learning](papers/citation.pdf). 

### Reviewing
- ACML 2017. Program Committee Member
- NIPS 2017. reviewer.
- ICML 2017. Program Committee Member.
- AIStats 2017, program committee member.
- CIKM 2016, program committee member (research and industry track).
- ICML 2016, reviewer. 
- NIPS 2016, reviewer.
- WWW 2015, 2016, pc member (on web search)
- The First Workshop on Heterogeneous Information Access at WSDM 2015, pc member.
- IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning(ADPRL) 2014, pc member.

