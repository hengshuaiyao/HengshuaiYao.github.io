## Welcome!

Hi, My name is Hengshuai Yao. Currently I am working at Huawei Canada leading a team of 13 developing RL and DL based robots. Before that, I was building an AI system for NCSoft in California. When doing my Ph.D at University of Alberta (2008--2014), I was working with Csaba Szepesvari, Rich Sutton, Dale Schuurmans, Davood Rafiei, Shalabh Bhatnagar and Chihoon Lee. I had spent two years working on a mobile app startup (6 team members) in Edmonton during and after my PhD studies.  

## Research

### Thesis
Yao, H. Model-based Reinforcement Learning with State and Action Abstractions. Ph.D thesis, 2015. pdf

### Action-model based RL

- Yao, H. and Szepesvari, Cs. Approximate Policy Iteration with Linear Action Models. Twenty-Sixth Conference on Artificial Intelligence. AAAI. Toronto, Canada. 2012. pdf
- Yao, H., Szepesvari, Cs., Pires, B. A., and Zhang, X. 2014. Pseudo-MDPs and Factored Linear Action Models. IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (IEEE ADPRL), Best student paper nomination, Orlando, Florida, USA. pdf

### Dyna-style Planning
Multi-step linear Dyna style planning. 

- Yao, H., Sutton, R. S., Bhatnagar, S., Diao, D., and Szepesvari, Cs. Dyna(k): A multi-step Dyna planning. Abstraction in Reinforcement Learning. Montreal, Canada. June 2009. pdf
- Yao, H., Bhatnagar, S., and Diao, D. Multi-step linear Dyna-style planning. Advances in Neural Information Processing Systems (NIPS) 22, Vancouver, BC, Canada. 2009. pdf
- Yao, H. Linear least-squares Dyna-style planning. Technical Report TR11-04, Department of Computing Science, University of Alberta. 2011.

### Off-policy Learning
- Yao, H. Off-policy learning with linear action models: an efficient "One-Collection-For-All-Solution". In workshop on "Planning and Acting with Uncertain Models" at the 28th ICML, Bellevue, Washington, USA. 2011. pdf

### Temporal Difference Learning

- Preconditioned TD learning. 
Yao, H., and Liu, Z-Q. Preconditioned temporal difference learning. The 25th International Conference on Machine learning (ICML), Helsinki, Finland. June 2008. pdf
- Yao, H., and Liu, Z-Q. Minimal residual approaches for policy evaluation in large sparse Markov chains. The Tenth International Symposium on Artificial Intelligence and Mathematics (ISAIM), Fort Lauderdale, USA. January 2008. pdf.
- Yao, H., Bhatnagar, S., and Szepesvari, Cs. Temporal difference learning by direct preconditioning. Multidisciplinary Symposium on Reinforcement Learning (MSRL), Montreal, Canada. June 2009. pdf
- Two-time scale update. 
Yao, H., Bhatnagar, S., and Szepesvari, Cs. LMS-2: towards an algorithm that is as cheap as LMS and almost as efficient as RLS. The Forty-eighth IEEE Control and Decision Conference (CDC), Shanghai, China. December 2009.pdf

### Options
- Yao, H., Szepesvari, Cs., Sutton, R., and Bhatnagar,S. 2014. Universal Option Models. NIPS. Montreal, Quebec, Canada. pdf

### Web Search
- Yao, H. and Schuurmans, D. 2013. Reinforcement Ranking. arXiv:1303.5988.pdf
- Lee, C., Yao, H., He, X., Su, C., and Chang, J-Y. 2014. A System to Predict Future Popularity: Learning to Classify. WWW (poster), Seol,Korea. pdf
- Yao, H. 2012. MaxRank: Discovering and Leveraging the Most Valuable Links for Ranking. arxiv 1210.1626. pdf
- Yao, H., Sutton R. and Rafiei D. A Study of Temporal Citation Count Prediction using Reinforcement Learning. pdf 

### Reviewing
- ACML 2017. Program Committee Member
- NIPS 2017. reviewer.
- ICML 2017. Program Committee Member.
- AIStats 2017, program committee member.
- CIKM 2016, program committee member (research and industry track).
- ICML 2016, reviewer. 
- NIPS 2016, reviewer.
- WWW 2015, 2016, pc member (on web search)
- The First Workshop on Heterogeneous Information Access at WSDM 2015, pc member.
- IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning(ADPRL) 2014, pc member.

